{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MMjHsM9TSmW"
      },
      "source": [
        "# data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49VIe1sP2LmK"
      },
      "source": [
        "## dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tXEhB-oDxf-p"
      },
      "outputs": [],
      "source": [
        "import lightning as L\n",
        "\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9yrm7XFBxebN"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, path, transform):\n",
        "        super().__init__()\n",
        "        self.path = Path(path)\n",
        "        self.transform = transform\n",
        "        self.data = list(self.path.glob(pattern='*.*'))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image = Image.open(fp=self.data[index]).convert(mode=\"RGB\")\n",
        "        return self.transform(image)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5zyQLQwhs7EV"
      },
      "outputs": [],
      "source": [
        "class CustomDataModule(L.LightningDataModule):\n",
        "    def __init__(self, train_dir, valid_dir, infer_dir, bench_dir, transform, batch_size=32, num_workers=4):\n",
        "        super().__init__()\n",
        "        self.train_dir = Path(train_dir)\n",
        "        self.valid_dir = Path(valid_dir)\n",
        "        self.infer_dir = Path(infer_dir)\n",
        "        self.bench_dir = Path(bench_dir)\n",
        "\n",
        "        self.transform = transform\n",
        "        self.batch_size = batch_size\n",
        "        self.num_workers = num_workers\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        self.train_datasets = self._set_dataset(path=self.train_dir)\n",
        "        self.valid_datasets = self._set_dataset(path=self.valid_dir)\n",
        "        self.bench_datasets = self._set_dataset(path=self.bench_dir)\n",
        "        self.infer_datasets = self._set_dataset(path=self.infer_dir)\n",
        "\n",
        "    def _set_dataset(self, path):\n",
        "        datasets = []\n",
        "        for folder in path.iterdir():\n",
        "            if folder.is_dir():\n",
        "                datasets.append(\n",
        "                    CustomDataset(\n",
        "                        path=folder,\n",
        "                        transform=self.transform\n",
        "                    )\n",
        "                )\n",
        "        return datasets\n",
        "\n",
        "    def _set_dataloader(self, datasets, concat=False, shuffle=False):\n",
        "        if concat:\n",
        "            dataloader = DataLoader(\n",
        "                dataset=ConcatDataset(datasets=datasets),\n",
        "                batch_size=self.batch_size,\n",
        "                shuffle=shuffle,\n",
        "                num_workers=self.num_workers,\n",
        "                pin_memory=True\n",
        "            )\n",
        "            return dataloader\n",
        "        else:\n",
        "            dataloaders = []\n",
        "            for dataset in datasets:\n",
        "                loader = DataLoader(\n",
        "                    dataset=dataset,\n",
        "                    batch_size=self.batch_size,\n",
        "                    shuffle=shuffle,\n",
        "                    num_workers=self.num_workers,\n",
        "                    pin_memory=True\n",
        "                )\n",
        "                dataloaders.append(loader)\n",
        "            return dataloaders\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return self._set_dataloader(datasets=self.train_datasets, concat=True, shuffle=True)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return self._set_dataloader(datasets=self.valid_datasets, concat=True)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return self._set_dataloader(datasets=self.bench_datasets)\n",
        "\n",
        "    def predict_dataloader(self):\n",
        "        return self._set_dataloader(datasets=self.infer_datasets)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-ofYHcU2P4U"
      },
      "source": [
        "## utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uohVFoIuyKbK"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yz-ob2QEyGx-"
      },
      "outputs": [],
      "source": [
        "class DataTransform:\n",
        "    def __init__(self, image_size=256):\n",
        "        self.image_size = image_size\n",
        "\n",
        "        self.transform = self._build_transform()\n",
        "\n",
        "    def _build_transform(self):\n",
        "        base = [\n",
        "            transforms.Resize(size=(self.image_size, self.image_size)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Lambda(lambd=lambda x: x.float()),\n",
        "        ]\n",
        "\n",
        "        return transforms.Compose(transforms=base)\n",
        "\n",
        "    def __call__(self, image):\n",
        "        return self.transform(img=image)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import cv2\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from scipy.ndimage import convolve\n",
        "from scipy.special import gamma\n",
        "from torchmetrics.image import (\n",
        "    PeakSignalNoiseRatio,\n",
        "    StructuralSimilarityIndexMeasure,\n",
        "    LearnedPerceptualImagePatchSimilarity,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ImageQualityMetrics(nn.Module):\n",
        "    def __init__(self, device=\"cuda\", data_range=1.0):\n",
        "        super().__init__()\n",
        "\n",
        "        self.device_type = device\n",
        "\n",
        "        # reference-based metrics\n",
        "        self.psnr = PeakSignalNoiseRatio(\n",
        "            data_range=data_range).to(device=device)\n",
        "        self.ssim = StructuralSimilarityIndexMeasure(\n",
        "            data_range=data_range).to(device=device)\n",
        "        self.lpips = LearnedPerceptualImagePatchSimilarity(\n",
        "            net_type='squeeze').to(device=device)\n",
        "\n",
        "        # for NIQE (dummy pristine dist)\n",
        "        self.niqe_stats = np.load(\n",
        "            file=\"utils/files/niqe_params.npz\", allow_pickle=True)\n",
        "        self.mu_pris_param = self.niqe_stats['mu_pris_param']\n",
        "        self.cov_pris_param = self.niqe_stats['cov_pris_param']\n",
        "        self.gaussian_window = self.niqe_stats['gaussian_window']\n",
        "\n",
        "    def forward(self, preds: torch.Tensor, targets: torch.Tensor):\n",
        "        preds = preds.to(device=self.device_type)\n",
        "        targets = targets.to(device=self.device_type)\n",
        "\n",
        "        return {\n",
        "            \"PSNR\": self.psnr(preds, targets).item(),\n",
        "            \"SSIM\": self.ssim(preds, targets).item(),\n",
        "            \"LPIPS\": self.lpips(preds, targets).squeeze().mean().item(),\n",
        "        }\n",
        "\n",
        "    def no_ref(self, preds: torch.Tensor):\n",
        "        preds = preds.to(device=self.device_type)\n",
        "        preds_np = preds.detach().cpu().numpy()\n",
        "        preds_np = np.clip(a=preds_np, a_min=0, a_max=1)\n",
        "\n",
        "        niqe_list = []\n",
        "        brisque_list = []\n",
        "\n",
        "        for img in preds_np:\n",
        "            # (C, H, W) → (H, W, C)\n",
        "            img_np = np.transpose(a=img, axes=(1, 2, 0))\n",
        "            img_np_uint8 = (img_np * 255).astype(dtype=np.uint8)\n",
        "\n",
        "            niqe = self._compute_niqe(img_np=img_np)\n",
        "            brisuqe = self._compute_brisque(img=img_np_uint8)\n",
        "\n",
        "            niqe_list.append(niqe)\n",
        "            brisque_list.append(brisuqe)\n",
        "\n",
        "        return {\n",
        "            \"NIQE\": float(x=np.mean(a=niqe_list)),\n",
        "            \"BRISQUE\": float(x=np.mean(a=brisque_list)),\n",
        "        }\n",
        "\n",
        "    def full(self, preds, targets):\n",
        "        ref_metrics = self.forward(preds=preds, targets=targets)\n",
        "        no_ref_metrics = self.no_ref(preds=preds)\n",
        "        return {**ref_metrics, **no_ref_metrics}\n",
        "\n",
        "    # --------------------------\n",
        "    # Custom no-reference metrics\n",
        "    # --------------------------\n",
        "\n",
        "    def _compute_niqe(self, img_np):\n",
        "        img = img_np.astype(np.float32)\n",
        "        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "        gray = gray.round()\n",
        "        return self._niqe(img=gray)\n",
        "\n",
        "    def _compute_brisque(self, img):\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "        brisque_score = cv2.quality.QualityBRISQUE_compute(\n",
        "            img, \"utils/files/brisque_model.yaml\", \"utils/files/brisque_range.yaml\")\n",
        "        return brisque_score\n",
        "\n",
        "    def _niqe(self, img, block_size_h=96, block_size_w=96):\n",
        "        assert img.ndim == 2\n",
        "        h, w = img.shape\n",
        "        num_block_h = math.floor(h / block_size_h)\n",
        "        num_block_w = math.floor(w / block_size_w)\n",
        "        img = img[0:num_block_h * block_size_h, 0:num_block_w * block_size_w]\n",
        "\n",
        "        distparam = []\n",
        "        for scale in (1, 2):\n",
        "            mu = convolve(\n",
        "                input=img, weights=self.gaussian_window, mode='nearest')\n",
        "            sigma = np.sqrt(np.abs(convolve(\n",
        "                input=np.square(img), weights=self.gaussian_window, mode='nearest') - np.square(mu)))\n",
        "            img_nomalized = (img - mu) / (sigma + 1)\n",
        "\n",
        "            feat = []\n",
        "            for idx_w in range(num_block_w):\n",
        "                for idx_h in range(num_block_h):\n",
        "                    block = img_nomalized[\n",
        "                        idx_h * block_size_h // scale:(idx_h + 1) * block_size_h // scale,\n",
        "                        idx_w * block_size_w // scale:(idx_w + 1) * block_size_w // scale\n",
        "                    ]\n",
        "                    feat.append(self._compute_feature(block=block))\n",
        "\n",
        "            distparam.append(np.array(object=feat))\n",
        "\n",
        "            if scale == 1:\n",
        "                img = cv2.resize(img / 255., dsize=(0, 0), fx=0.5,\n",
        "                                 fy=0.5, interpolation=cv2.INTER_CUBIC) * 255.\n",
        "\n",
        "        distparam = np.concatenate(distparam, axis=1)\n",
        "        mu_distparam = np.nanmean(a=distparam, axis=0)\n",
        "        distparam_no_nan = distparam[~np.isnan(distparam).any(axis=1)]\n",
        "        cov_distparam = np.cov(m=distparam_no_nan, rowvar=False)\n",
        "\n",
        "        invcov_param = np.linalg.pinv(\n",
        "            (self.cov_pris_param + cov_distparam) / 2)\n",
        "        quality = np.matmul(\n",
        "            np.matmul((self.mu_pris_param - mu_distparam), invcov_param),\n",
        "            np.transpose(a=(self.mu_pris_param - mu_distparam))\n",
        "        )\n",
        "        return float(x=np.sqrt(quality))\n",
        "\n",
        "    def _compute_feature(self, block):\n",
        "        def estimate_aggd_param(block):\n",
        "            block = block.flatten()\n",
        "            gam = np.arange(start=0.2, stop=10.001, step=0.001)\n",
        "            gam_reciprocal = np.reciprocal(gam)\n",
        "            r_gam = np.square(gamma(gam_reciprocal * 2)) / (\n",
        "                gamma(gam_reciprocal) * gamma(gam_reciprocal * 3))\n",
        "\n",
        "            left_std = np.sqrt(np.mean(block[block < 0]**2))\n",
        "            right_std = np.sqrt(np.mean(block[block > 0]**2))\n",
        "            gammahat = left_std / right_std\n",
        "            rhat = (np.mean(np.abs(block)))**2 / np.mean(block**2)\n",
        "            rhatnorm = (rhat * (gammahat**3 + 1) *\n",
        "                        (gammahat + 1)) / ((gammahat**2 + 1)**2)\n",
        "            array_position = np.argmin(a=(r_gam - rhatnorm)**2)\n",
        "            alpha = gam[array_position]\n",
        "            beta_l = left_std * np.sqrt(gamma(1 / alpha) / gamma(3 / alpha))\n",
        "            beta_r = right_std * np.sqrt(gamma(1 / alpha) / gamma(3 / alpha))\n",
        "            return (alpha, beta_l, beta_r)\n",
        "\n",
        "        feat = []\n",
        "        alpha, beta_l, beta_r = estimate_aggd_param(block=block)\n",
        "        feat.extend([alpha, (beta_l + beta_r) / 2])\n",
        "        shifts = [[0, 1], [1, 0], [1, 1], [1, -1]]\n",
        "        for shift in shifts:\n",
        "            shifted = np.roll(a=block, shift=shift, axis=(0, 1))\n",
        "            alpha, beta_l, beta_r = estimate_aggd_param(block=block * shifted)\n",
        "            mean = (beta_r - beta_l) * (gamma(2 / alpha) / gamma(1 / alpha))\n",
        "            feat.extend([alpha, mean, beta_l, beta_r])\n",
        "        return feat\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from torchvision.utils import save_image\n",
        "from torchinfo import summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_dirs(path: str | Path):\n",
        "    path = Path(path)\n",
        "    path.mkdir(parents=True, exist_ok=True)\n",
        "    return path\n",
        "\n",
        "\n",
        "def print_metrics(metrics: dict, prefix: str = \"\"):\n",
        "    for k, v in metrics.items():\n",
        "        print(f\"{prefix}{k}: {v:.4f}\")\n",
        "\n",
        "\n",
        "def save_images(results, save_dir, prefix=\"infer\", ext=\"png\"):\n",
        "    for i, datasets in enumerate(iterable=results):\n",
        "        save_path = make_dirs(path=f\"{save_dir}/batch{i+1}\")\n",
        "        for ii, batch in enumerate(iterable=datasets):\n",
        "            save_image(\n",
        "                tensor=batch,\n",
        "                fp=save_path / f\"{prefix}_{ii:04d}.{ext}\",\n",
        "                nrow=8,\n",
        "                padding=2,\n",
        "                normalize=True,\n",
        "                value_range=(0, 1)\n",
        "            )\n",
        "\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "def summarize_model(model, input_size):\n",
        "    return summary(model=model, input_size=input_size, depth=3, col_names=[\"input_size\", \"output_size\", \"num_params\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lh4S0MOTaaJ"
      },
      "source": [
        "# engine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qA5HkZA-2VRM"
      },
      "source": [
        "## trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WA-AweC9yQbI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from lightning import Trainer, seed_everything\n",
        "from lightning.pytorch.callbacks import *\n",
        "from lightning.pytorch.loggers import TensorBoardLogger\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYf9FGjfyTUg"
      },
      "outputs": [],
      "source": [
        "class LightningTrainer:\n",
        "    def __init__(self, model, hparams: dict, ckpt: Path = None, transform=None):\n",
        "        self.hparams = hparams\n",
        "        self.transform = transform if transform else DataTransform()\n",
        "        seed_everything(seed=hparams[\"seed\"], workers=True)\n",
        "\n",
        "        # --- 모델 정의\n",
        "        if ckpt:\n",
        "            self.model = model.load_from_checkpoint(\n",
        "                checkpoint_path=ckpt,\n",
        "                map_location=\"cuda\",\n",
        "            )\n",
        "            self.ckpt = ckpt\n",
        "        else:\n",
        "            self.model = model\n",
        "\n",
        "        # --- DataModule 정의\n",
        "        self.datamodule = self._build_datamodule()\n",
        "\n",
        "        # --- 로깅 설정\n",
        "        self.logger = self._build_logger()\n",
        "\n",
        "        # --- 콜백 정의\n",
        "        self.callbacks = self._build_callbacks()\n",
        "\n",
        "        # --- Lightning Trainer 정의\n",
        "        self.trainer = Trainer(\n",
        "            max_epochs=hparams[\"epochs\"],\n",
        "            accelerator=\"gpu\",\n",
        "            devices=1,\n",
        "            precision=\"16\",\n",
        "            logger=self.logger,\n",
        "            callbacks=self.callbacks,\n",
        "            log_every_n_steps=5,\n",
        "        )\n",
        "\n",
        "    def _build_datamodule(self):\n",
        "        return CustomDataModule(\n",
        "            train_dir=self.hparams[\"train_data_path\"],\n",
        "            valid_dir=self.hparams[\"valid_data_path\"],\n",
        "            infer_dir=self.hparams[\"infer_data_path\"],\n",
        "            bench_dir=self.hparams[\"bench_data_path\"],\n",
        "            transform=DataTransform(image_size=self.hparams[\"image_size\"]),\n",
        "            batch_size=self.hparams[\"batch_size\"],\n",
        "            num_workers=int(os.cpu_count() * 0.9),\n",
        "        )\n",
        "\n",
        "    def _build_logger(self):\n",
        "        return TensorBoardLogger(\n",
        "            save_dir=self.hparams[\"log_dir\"],\n",
        "            name=self.hparams[\"experiment_name\"]\n",
        "        )\n",
        "\n",
        "    def _build_callbacks(self):\n",
        "        return [\n",
        "            ModelCheckpoint(\n",
        "                monitor=\"valid/06_total\",\n",
        "                save_top_k=1,\n",
        "                mode=\"min\",\n",
        "                filename=\"best-{epoch:02d}\",\n",
        "            ),\n",
        "            ModelCheckpoint(\n",
        "                every_n_epochs=10,\n",
        "                save_top_k=-1,  # 모두 저장\n",
        "                filename=\"epoch-{epoch:02d}\",\n",
        "            ),\n",
        "            EarlyStopping(\n",
        "                monitor=\"valid/06_total\",\n",
        "                patience=10,\n",
        "                mode=\"min\",\n",
        "                verbose=True,\n",
        "            ),\n",
        "            LearningRateMonitor(logging_interval=\"step\"),\n",
        "            RichProgressBar(),\n",
        "        ]\n",
        "\n",
        "    def run(self):\n",
        "        print(\"[INFO] Start training...\")\n",
        "        self.trainer.fit(\n",
        "            model=self.model,\n",
        "            datamodule=self.datamodule\n",
        "        )\n",
        "        print(\"[INFO] Training completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vN4DsMQS2bfB"
      },
      "source": [
        "## validater"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-TcPiApZ1EBm"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "from lightning import Trainer\n",
        "from pathlib import Path\n",
        "from tqdm.auto import tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1lxZzwSb0oM3"
      },
      "outputs": [],
      "source": [
        "class LightningValidater:\n",
        "    def __init__(self, model, trainer: Trainer, ckpt: Path, hparams: dict):\n",
        "        self.hparams = hparams\n",
        "\n",
        "        # --- 모델 정의\n",
        "        if ckpt:\n",
        "            self.model = model.load_from_checkpoint(\n",
        "                checkpoint_path=ckpt,\n",
        "                map_location=\"cuda\",\n",
        "            )\n",
        "            self.ckpt = ckpt\n",
        "        else:\n",
        "            self.model = model\n",
        "            self.ckpt = \"best\"\n",
        "\n",
        "        # --- Lightning Trainer 정의\n",
        "        self.trainer = trainer\n",
        "\n",
        "        # --- DataModule 정의\n",
        "        self.datamodule = self._build_datamodule()\n",
        "\n",
        "    def _build_datamodule(self):\n",
        "        return CustomDataModule(\n",
        "            train_dir=self.hparams[\"train_data_path\"],\n",
        "            valid_dir=self.hparams[\"valid_data_path\"],\n",
        "            infer_dir=self.hparams[\"infer_data_path\"],\n",
        "            bench_dir=self.hparams[\"bench_data_path\"],\n",
        "            transform=DataTransform(image_size=self.hparams[\"image_size\"]),\n",
        "            batch_size=self.hparams[\"batch_size\"],\n",
        "            num_workers=int(os.cpu_count() * 0.9),\n",
        "        )\n",
        "\n",
        "    def run(self):\n",
        "        print(\"[INFO] Start validating...\")\n",
        "        results = self.trainer.validate(\n",
        "            model=self.model,\n",
        "            datamodule=self.datamodule,\n",
        "            ckpt_path=self.ckpt\n",
        "        )\n",
        "        print(\"[VALIDATION RESULT]\")\n",
        "        for res in tqdm(results):\n",
        "            print(res)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-XEQ7Ab2g_L"
      },
      "source": [
        "## inferencer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K_fzZBK51Hi5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import yaml\n",
        "\n",
        "from lightning import Trainer\n",
        "from pathlib import Path\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Smv5YDwQ1I0U"
      },
      "outputs": [],
      "source": [
        "class LightningInferencer:\n",
        "    def __init__(self, model, trainer: Trainer, ckpt: Path, hparams: Path):\n",
        "        with open(file=hparams) as f:\n",
        "            hparams = yaml.load(stream=f, Loader=yaml.FullLoader)\n",
        "        self.hparams = hparams\n",
        "\n",
        "        if ckpt:\n",
        "            self.model = model.load_from_checkpoint(\n",
        "                checkpoint_path=str(object=ckpt),\n",
        "                map_location=\"cuda\",\n",
        "            )\n",
        "            self.ckpt = ckpt\n",
        "        else:\n",
        "            self.model = model\n",
        "            self.ckpt = \"best\"\n",
        "\n",
        "        # --- Lightning Trainer 정의\n",
        "        self.trainer = trainer\n",
        "\n",
        "        # --- DataModule 정의\n",
        "        self.datamodule = self._build_datamodule()\n",
        "\n",
        "        self.save_dir = ckpt.parents[1] / hparams[\"inference\"]\n",
        "        print(f\"save_dir: {self.save_dir}\")\n",
        "\n",
        "    def _build_datamodule(self):\n",
        "        return CustomDataModule(\n",
        "            train_dir=self.hparams[\"train_data_path\"],\n",
        "            valid_dir=self.hparams[\"valid_data_path\"],\n",
        "            infer_dir=self.hparams[\"infer_data_path\"],\n",
        "            bench_dir=self.hparams[\"bench_data_path\"],\n",
        "            transform=DataTransform(image_size=self.hparams[\"image_size\"]),\n",
        "            batch_size=self.hparams[\"batch_size\"],\n",
        "            num_workers=int(os.cpu_count() * 0.9),\n",
        "        )\n",
        "\n",
        "    def run(self):\n",
        "        print(\"[INFO] Start training...\")\n",
        "        results = self.trainer.predict(\n",
        "            model=self.model,\n",
        "            datamodule=self.datamodule,\n",
        "            ckpt_path=self.ckpt\n",
        "        )\n",
        "        save_images(results=results, save_dir=self.save_dir)\n",
        "        print(\"[INFO] Inference completed.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YobhDE1g2lC3"
      },
      "source": [
        "## benchmarker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_f7eA4M1P8Y"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "from lightning import Trainer\n",
        "from pathlib import Path\n",
        "from tqdm.auto import tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wCpVMRNi1Sjx"
      },
      "outputs": [],
      "source": [
        "class LightningBenchmarker:\n",
        "    def __init__(self, model, trainer: Trainer, ckpt: Path, hparams: dict):\n",
        "        self.hparams = hparams\n",
        "\n",
        "        if ckpt:\n",
        "            self.model = model.load_from_checkpoint(\n",
        "                checkpoint_path=ckpt,\n",
        "                map_location=\"cuda\",\n",
        "            )\n",
        "            self.ckpt = ckpt\n",
        "        else:\n",
        "            self.model = model\n",
        "            self.ckpt = \"best\"\n",
        "\n",
        "        # --- Lightning Trainer 정의\n",
        "        self.trainer = trainer\n",
        "\n",
        "        # --- DataModule 정의\n",
        "        self.datamodule = self._build_datamodule()\n",
        "\n",
        "        # --- 평가 메트릭 정의\n",
        "        self.metric = ImageQualityMetrics(device=\"cuda\")\n",
        "        self.metric.eval()\n",
        "\n",
        "    def _build_datamodule(self):\n",
        "        datamodule = CustomDataModule(\n",
        "            train_dir=self.hparams[\"train_data_path\"],\n",
        "            valid_dir=self.hparams[\"valid_data_path\"],\n",
        "            infer_dir=self.hparams[\"infer_data_path\"],\n",
        "            bench_dir=self.hparams[\"bench_data_path\"],\n",
        "            transform=DataTransform(image_size=self.hparams[\"image_size\"]),\n",
        "            batch_size=self.hparams[\"batch_size\"],\n",
        "            num_workers=int(os.cpu_count() * 0.9),\n",
        "        )\n",
        "        datamodule.setup()  # 벤치마크 데이터셋 사용 위해 미리 세팅\n",
        "        return datamodule\n",
        "\n",
        "    def run(self):\n",
        "        print(\"[INFO] Start benchmarking image quality metrics...\")\n",
        "\n",
        "        outputs = self.trainer.test(\n",
        "            model=self.model,\n",
        "            datamodule=self.datamodule,\n",
        "            ckpt_path=self.ckpt\n",
        "        )\n",
        "        print(\"outputs\", outputs)\n",
        "        print(\"\\n[FINAL BENCHMARK RESULT]\")\n",
        "        for k, v in tqdm(outputs.items()):\n",
        "            print(f\"{k}: {v:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QFt6KdDTjPW"
      },
      "source": [
        "# model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQXFZQKy2pao"
      },
      "source": [
        "## block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nV7wTXp52sq_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tx2P0D9VTqcH"
      },
      "source": [
        "### study"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ksWXAbzx25Pc"
      },
      "outputs": [],
      "source": [
        "class RGB2YCrCb(nn.Module):\n",
        "    def __init__(self, offset=0.5, ):\n",
        "        super().__init__()\n",
        "        self.offset = offset\n",
        "        self.register_buffer(\n",
        "            name='weights',\n",
        "            tensor=torch.tensor(\n",
        "                data=[\n",
        "                    [0.299,  0.587,  0.114],   # Y\n",
        "                    [0.713, -0.713,  0.000],   # Cr\n",
        "                    [0.000, -0.564,  0.564],   # Cb\n",
        "                ],\n",
        "                dtype=torch.float32\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.einsum('bchw,oc->bohw', x, self.weights)\n",
        "\n",
        "        Y = out[:, 0:1, :, :]  # (B,1,H,W)\n",
        "        Cr = out[:, 1:2, :, :] + self.offset  # (B,1,H,W)\n",
        "        Cb = out[:, 2:3, :, :] + self.offset  # (B,1,H,W)\n",
        "        return Y, Cr, Cb\n",
        "\n",
        "\n",
        "class YCrCb2RGB(nn.Module):\n",
        "    def __init__(self, offset=0.5,):\n",
        "        super().__init__()\n",
        "        self.offset = offset\n",
        "        self.register_buffer(\n",
        "            name='weights',\n",
        "            tensor=torch.tensor(\n",
        "                data=[\n",
        "                    [1.000, 1.403, 0.000],\n",
        "                    [1.000, -0.714, -0.344],\n",
        "                    [1.000, 0.000, 1.773]\n",
        "                ],\n",
        "                dtype=torch.float32\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def forward(self, Y, Cr, Cb):\n",
        "        Cr = Cr - self.offset\n",
        "        Cb = Cb - self.offset\n",
        "\n",
        "        inputs = torch.cat(tensors=[Y, Cr, Cb], dim=1)  # (B,3,H,W)\n",
        "\n",
        "        rgb = torch.einsum('bchw,oc->bohw', inputs, self.weights)\n",
        "        return rgb\n",
        "\n",
        "\n",
        "class HomomorphicSeparation(nn.Module):\n",
        "    def __init__(self, size=128, cutoff=0.1, eps=1e-6):\n",
        "        super().__init__()\n",
        "        self.size = size\n",
        "        self.cutoff = cutoff\n",
        "        self.eps = eps\n",
        "\n",
        "        self.register_buffer(\n",
        "            name='filter_mask',\n",
        "            tensor=self._build_gaussian_low_pass_filter(\n",
        "                size=self.size,\n",
        "                cutoff=self.cutoff\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def _build_gaussian_low_pass_filter(self, size, cutoff):\n",
        "        coord = torch.linspace(start=-1, end=1, steps=size)\n",
        "        y, x = torch.meshgrid(coord, coord, indexing='ij')\n",
        "        d = torch.sqrt(input=x ** 2 + y ** 2)\n",
        "        filter = torch.exp(input=-(d**2) / (2*cutoff**2))\n",
        "        return filter\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "\n",
        "        # 1. log 변환\n",
        "        x_log = torch.log(input=x + self.eps)  # (B, 1, H, W)\n",
        "\n",
        "        # 2. FFT\n",
        "        x_fft = torch.fft.fft2(x_log.squeeze(dim=1))  # (B, H, W)\n",
        "\n",
        "        # 3. Low-pass / High-pass 분리\n",
        "        filter_mask = self.filter_mask.unsqueeze(\n",
        "            0).expand(B, -1, -1)  # (B, H, W)\n",
        "\n",
        "        low_fft = x_fft * filter_mask\n",
        "        high_fft = x_fft * (1 - filter_mask)\n",
        "\n",
        "        # 4. IFFT 후 real 값 추출\n",
        "        low_spatial = torch.real(\n",
        "            input=torch.fft.ifft2(\n",
        "                low_fft\n",
        "            )\n",
        "        ).unsqueeze(dim=1)  # (B,1,H,W)\n",
        "        high_spatial = torch.real(\n",
        "            input=torch.fft.ifft2(\n",
        "                high_fft\n",
        "            )\n",
        "        ).unsqueeze(dim=1)  # (B,1,H,W)\n",
        "\n",
        "        # 5. exp 복원\n",
        "        illumination = torch.exp(input=low_spatial)\n",
        "        detail = torch.exp(input=high_spatial)\n",
        "\n",
        "        return illumination, detail\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oruflwnMTso-"
      },
      "source": [
        "### embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wNK7Llu92_RE"
      },
      "outputs": [],
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, image_size=512, in_channels=1, embed_dim=768, patch_size=4, bias=True):\n",
        "        super().__init__()\n",
        "        self.image_size = image_size\n",
        "        self.grid_size = self.image_size // patch_size\n",
        "        self.num_patches = self.grid_size ** 2\n",
        "        self.in_channels = in_channels\n",
        "        self.embed_dim = embed_dim\n",
        "        self.patch_size = patch_size\n",
        "        self.bias = bias\n",
        "\n",
        "        self.proj = nn.Conv2d(\n",
        "            in_channels=self.in_channels,\n",
        "            out_channels=self.embed_dim,\n",
        "            kernel_size=self.patch_size,\n",
        "            stride=self.patch_size,\n",
        "            bias=self.bias\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (B, embed_dim, image_size/patch_size, image_size/patch_size) -> (B, 768, 128, 128)\n",
        "        x = self.proj(x)\n",
        "        # (B, embed_dim, (image_size/patch_size)**2) -> (B, 768, 16384)\n",
        "        x = x.flatten(2)\n",
        "        # (B, (image_size/patch_size)**2, embed_dim) -> (B, 16384, 768\n",
        "        x = x.transpose(1, 2)\n",
        "        return x\n",
        "\n",
        "\n",
        "class PositionalEmbedding(nn.Module):\n",
        "    def __init__(self, embed_dim=768, size=128):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.size = size\n",
        "\n",
        "        self.device = torch.device(device='cuda')\n",
        "\n",
        "    def _build_sincos_embedding(self, size):\n",
        "        grid = torch.linspace(start=0, end=1, steps=size, device=self.device)\n",
        "        grid_y, grid_x = torch.meshgrid(grid, grid, indexing='ij')  # (H, W)\n",
        "\n",
        "        grid = torch.stack(tensors=[grid_y, grid_x], dim=0)  # (2, H, W)\n",
        "\n",
        "        pos_embed = self._get_2d_sincos_pos_embed_from_grid(\n",
        "            embed_dim=self.embed_dim,\n",
        "            grid=grid\n",
        "        )\n",
        "        pos_embed = pos_embed.view(1, size * size, self.embed_dim)  # (1, N, D)\n",
        "        return pos_embed\n",
        "\n",
        "    def _get_1d_sincos_pos_embed_from_grid(self, embed_dim, pos):\n",
        "        omega = torch.arange(\n",
        "            end=embed_dim // 2, dtype=torch.float32, device=self.device)\n",
        "        omega = 1. / (10000 ** (omega / (embed_dim / 2)))\n",
        "\n",
        "        pos = pos.flatten()  # (H*W,)\n",
        "        out = torch.einsum('m,d->md', pos, omega)  # (H*W, embed_dim//2)\n",
        "\n",
        "        emb_sin = torch.sin(input=out)\n",
        "        emb_cos = torch.cos(input=out)\n",
        "\n",
        "        return torch.cat(tensors=[emb_sin, emb_cos], dim=1)  # (H*W, embed_dim)\n",
        "\n",
        "    def _get_2d_sincos_pos_embed_from_grid(self, embed_dim, grid):\n",
        "        emb_h = self._get_1d_sincos_pos_embed_from_grid(\n",
        "            embed_dim=embed_dim // 2, pos=grid[0])  # y\n",
        "        emb_w = self._get_1d_sincos_pos_embed_from_grid(\n",
        "            embed_dim=embed_dim // 2, pos=grid[1])  # x\n",
        "        return torch.cat(tensors=[emb_h, emb_w], dim=1)  # (H*W, embed_dim)\n",
        "\n",
        "    def forward(self):\n",
        "        pos_embed = self._build_sincos_embedding(size=self.size)\n",
        "        return pos_embed\n",
        "\n",
        "\n",
        "class TimeEmbedding(nn.Module):\n",
        "    def __init__(self, hidden_size=768, embed_dim=256, max_period=10000, bias=True):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embed_dim = embed_dim\n",
        "        self.max_period = max_period\n",
        "        self.bias = bias\n",
        "\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(\n",
        "                in_features=self.embed_dim,\n",
        "                out_features=self.hidden_size,\n",
        "                bias=self.bias\n",
        "            ),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(\n",
        "                in_features=self.hidden_size,\n",
        "                out_features=self.hidden_size,\n",
        "                bias=self.bias\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def timestep_embedding(self, t, embed_dim):\n",
        "        half_dim = embed_dim // 2\n",
        "        freqs = torch.exp(\n",
        "            input=math.log(self.max_period) * torch.arange(\n",
        "                end=half_dim,\n",
        "                dtype=t.dtype,\n",
        "                device=t.device\n",
        "            ) / half_dim\n",
        "        )\n",
        "        args = t[:, None].float() * freqs[None, :]\n",
        "        emb = torch.cat(\n",
        "            tensors=[\n",
        "                torch.cos(input=args),\n",
        "                torch.sin(input=args)\n",
        "            ],\n",
        "            dim=1\n",
        "        )\n",
        "        return emb\n",
        "\n",
        "    def forward(self, t):\n",
        "        t_freq = self.timestep_embedding(t=t, embed_dim=self.embed_dim)\n",
        "        t_emb = self.mlp(t_freq)\n",
        "        return t_emb  # (B, 768)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34F0RCw7Tuuz"
      },
      "source": [
        "### attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TsnpUQEx4mF_"
      },
      "outputs": [],
      "source": [
        "class MultiLayerPerceptron(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            in_features,\n",
        "            hidden_features=None,\n",
        "            out_features=None,\n",
        "            bias=True,\n",
        "            drop=0.,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features or in_features\n",
        "        self.hidden_features = hidden_features or in_features\n",
        "\n",
        "        self.fc1 = nn.Linear(\n",
        "            in_features=in_features,\n",
        "            out_features=self.hidden_features,\n",
        "            bias=bias\n",
        "        )\n",
        "        self.act = nn.GELU(\n",
        "            approximate=\"tanh\"\n",
        "        )\n",
        "        self.drop1 = nn.Dropout(\n",
        "            p=drop\n",
        "        )\n",
        "        self.fc2 = nn.Linear(\n",
        "            in_features=self.hidden_features,\n",
        "            out_features=self.out_features,\n",
        "            bias=bias\n",
        "        )\n",
        "        self.drop2 = nn.Dropout(\n",
        "            p=drop\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0.):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = dim // num_heads\n",
        "        self.scale = self.head_dim ** -0.5\n",
        "\n",
        "        self.qkv = nn.Linear(\n",
        "            in_features=dim,\n",
        "            out_features=dim * 3,\n",
        "            bias=qkv_bias\n",
        "        )\n",
        "        self.attn_drop = nn.Dropout(p=attn_drop)\n",
        "        self.proj = nn.Linear(\n",
        "            in_features=dim,\n",
        "            out_features=dim\n",
        "        )\n",
        "        self.proj_drop = nn.Dropout(p=proj_drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C //\n",
        "                                  self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv.unbind(0)\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxTFJ-84UEi_"
      },
      "source": [
        "### core"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "itD5KeVw8jRM"
      },
      "outputs": [],
      "source": [
        "class DiTBlock(nn.Module):\n",
        "    def __init__(self, hidden_size, num_heads, mlp_ratio=4.0):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_heads = num_heads\n",
        "        self.mlp_ratio = mlp_ratio\n",
        "        self.mlp_hidden_dim = int(self.hidden_size * self.mlp_ratio)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(\n",
        "            normalized_shape=self.hidden_size,\n",
        "            elementwise_affine=False,\n",
        "            eps=1e-6\n",
        "        )\n",
        "        self.attn = Attention(\n",
        "            dim=self.hidden_size,\n",
        "            num_heads=self.num_heads,\n",
        "            qkv_bias=True\n",
        "        )\n",
        "        self.norm2 = nn.LayerNorm(\n",
        "            normalized_shape=self.hidden_size,\n",
        "            elementwise_affine=False,\n",
        "            eps=1e-6\n",
        "        )\n",
        "        self.mlp = MultiLayerPerceptron(\n",
        "            in_features=self.hidden_size,\n",
        "            hidden_features=self.mlp_hidden_dim,\n",
        "            drop=0\n",
        "        )\n",
        "        self.adaLN_modulation = nn.Sequential(\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(\n",
        "                in_features=self.hidden_size,\n",
        "                out_features=6 * self.hidden_size,\n",
        "                bias=True\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def forward(self, x, c):\n",
        "        shift_msa, scale_msa, gate_msa, shift_mlp, scale_mlp, gate_mlp = self.adaLN_modulation(\n",
        "            c).chunk(6, dim=1)\n",
        "        x = self.norm1(x) * (1 + scale_msa.unsqueeze(1)) + \\\n",
        "            shift_msa.unsqueeze(1)\n",
        "        x = x + gate_msa.unsqueeze(1) * self.attn(x)\n",
        "        x = self.norm2(x) * (1 + scale_mlp.unsqueeze(1)) + \\\n",
        "            shift_mlp.unsqueeze(1)\n",
        "        x = x + gate_mlp.unsqueeze(1) * self.mlp(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class FinalLayer(nn.Module):\n",
        "    def __init__(self, hidden_size, patch_size, out_channels):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.patch_size = patch_size\n",
        "        self.out_channels = out_channels\n",
        "\n",
        "        self.norm_final = nn.LayerNorm(\n",
        "            normalized_shape=self.hidden_size,\n",
        "            elementwise_affine=False,\n",
        "            eps=1e-6\n",
        "        )\n",
        "        self.linear = nn.Linear(\n",
        "            in_features=self.hidden_size,\n",
        "            out_features=self.patch_size * self.patch_size * self.out_channels,\n",
        "            bias=True\n",
        "        )\n",
        "        self.adaLN_modulation = nn.Sequential(\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(\n",
        "                in_features=self.hidden_size,\n",
        "                out_features=2 * self.hidden_size,\n",
        "                bias=True\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def forward(self, x, c):\n",
        "        shift, scale = self.adaLN_modulation(c).chunk(2, dim=1)\n",
        "        x = self.norm_final(x) * (1 + scale.unsqueeze(1)) + shift.unsqueeze(1)\n",
        "        x = self.linear(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZGrCpoeQoHB"
      },
      "source": [
        "## losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision.models import vgg16\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### basic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9fBQncHKKBy5"
      },
      "outputs": [],
      "source": [
        "class L_col(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean_rgb = torch.mean(\n",
        "            input=x,\n",
        "            dim=[2, 3],\n",
        "            keepdim=True\n",
        "        )\n",
        "        mr, mg, mb = torch.split(\n",
        "            tensor=mean_rgb,\n",
        "            split_size_or_sections=1,\n",
        "            dim=1\n",
        "        )\n",
        "\n",
        "        Drg = torch.pow(input=mr - mg, exponent=2)\n",
        "        Drb = torch.pow(input=mr - mb, exponent=2)\n",
        "        Dgb = torch.pow(input=mb - mg, exponent=2)\n",
        "\n",
        "        c = torch.pow(input=torch.pow(input=Drg, exponent=2) + torch.pow(input=Drb,\n",
        "                      exponent=2) + torch.pow(input=Dgb, exponent=2), exponent=0.5)\n",
        "        return c\n",
        "\n",
        "\n",
        "class L_spa(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        kernel_l = torch.FloatTensor(\n",
        "            [[0, 0, 0], [-1, 1, 0], [0, 0, 0]]).cuda().unsqueeze(dim=0).unsqueeze(dim=0)\n",
        "        kernel_r = torch.FloatTensor(\n",
        "            [[0, 0, 0], [0, 1, -1], [0, 0, 0]]).cuda().unsqueeze(dim=0).unsqueeze(dim=0)\n",
        "        kernel_u = torch.FloatTensor(\n",
        "            [[0, -1, 0], [0, 1, 0], [0, 0, 0]]).cuda().unsqueeze(dim=0).unsqueeze(dim=0)\n",
        "        kernel_d = torch.FloatTensor(\n",
        "            [[0, 0, 0], [0, 1, 0], [0, -1, 0]]).cuda().unsqueeze(dim=0).unsqueeze(dim=0)\n",
        "\n",
        "        self.weight_l = nn.Parameter(data=kernel_l, requires_grad=False)\n",
        "        self.weight_r = nn.Parameter(data=kernel_r, requires_grad=False)\n",
        "        self.weight_u = nn.Parameter(data=kernel_u, requires_grad=False)\n",
        "        self.weight_d = nn.Parameter(data=kernel_d, requires_grad=False)\n",
        "        self.pool = nn.AvgPool2d(kernel_size=4)\n",
        "\n",
        "    def forward(self, org, enh):\n",
        "        org_mean = torch.mean(input=org, dim=1, keepdim=True)\n",
        "        enh_mean = torch.mean(input=enh, dim=1, keepdim=True)\n",
        "\n",
        "        org_pool = self.pool(org_mean)\n",
        "        enh_pool = self.pool(enh_mean)\n",
        "\n",
        "        D_org_l = F.conv2d(input=org_pool, weight=self.weight_l, padding=1)\n",
        "        D_org_r = F.conv2d(input=org_pool, weight=self.weight_r, padding=1)\n",
        "        D_org_u = F.conv2d(input=org_pool, weight=self.weight_u, padding=1)\n",
        "        D_org_d = F.conv2d(input=org_pool, weight=self.weight_d, padding=1)\n",
        "\n",
        "        D_enh_l = F.conv2d(input=enh_pool, weight=self.weight_l, padding=1)\n",
        "        D_enh_r = F.conv2d(input=enh_pool, weight=self.weight_r, padding=1)\n",
        "        D_enh_u = F.conv2d(input=enh_pool, weight=self.weight_u, padding=1)\n",
        "        D_enh_d = F.conv2d(input=enh_pool, weight=self.weight_d, padding=1)\n",
        "\n",
        "        D_l = torch.pow(input=D_org_l - D_enh_l, exponent=2)\n",
        "        D_r = torch.pow(input=D_org_r - D_enh_r, exponent=2)\n",
        "        D_u = torch.pow(input=D_org_u - D_enh_u, exponent=2)\n",
        "        D_d = torch.pow(input=D_org_d - D_enh_d, exponent=2)\n",
        "\n",
        "        s = (D_l + D_r + D_u + D_d)\n",
        "        return s\n",
        "\n",
        "\n",
        "class L_exp(nn.Module):\n",
        "    def __init__(self, patch_size=16, mean_val=0.6):\n",
        "        super().__init__()\n",
        "        self.pool = nn.AvgPool2d(kernel_size=patch_size)\n",
        "        self.mean_val = mean_val\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.mean(input=x, dim=1, keepdim=True)\n",
        "        mean = self.pool(x)\n",
        "\n",
        "        e = torch.mean(\n",
        "            input=torch.pow(\n",
        "                input=mean - torch.FloatTensor(\n",
        "                    [self.mean_val]\n",
        "                ).cuda(),\n",
        "                exponent=2\n",
        "            )\n",
        "        )\n",
        "        return e\n",
        "\n",
        "\n",
        "class L_TV(nn.Module):\n",
        "    def __init__(self, weight=1):\n",
        "        super().__init__()\n",
        "        self.weight = weight\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, h, w = x.shape\n",
        "\n",
        "        count_h = (h - 1) * w\n",
        "        count_w = h * (w - 1)\n",
        "\n",
        "        h_tv = torch.pow(\n",
        "            input=(x[:, :, 1:, :] - x[:, :, :h - 1, :]), exponent=2).sum()\n",
        "        w_tv = torch.pow(\n",
        "            input=(x[:, :, :, 1:] - x[:, :, :, :w - 1]), exponent=2).sum()\n",
        "\n",
        "        t = self.weight * 2 * ((h_tv / count_h) + (w_tv / count_w)) / b\n",
        "        return t\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Feature Attribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class L_bri(nn.Module):\n",
        "    def __init__(self, timestep_range=10000, weight=1.):\n",
        "        super().__init__()\n",
        "        self.timestep_range = timestep_range\n",
        "        self.weight = weight\n",
        "\n",
        "    def forward(self, i_x, n_Y, t):\n",
        "        mean_i_x = i_x.mean(dim=[1, 2, 3])\n",
        "        mean_n_Y = n_Y.mean(dim=[1, 2, 3])\n",
        "\n",
        "        target_increase = t.float() / self.timestep_range\n",
        "        actual_increase = mean_n_Y - mean_i_x\n",
        "\n",
        "        loss = F.mse_loss(input=actual_increase, target=target_increase)\n",
        "        b = self.weight * loss\n",
        "        return b\n",
        "\n",
        "\n",
        "class L_mod(nn.Module):\n",
        "    def __init__(self, weight=1.):\n",
        "        super().__init__()\n",
        "        self.weight = weight\n",
        "\n",
        "    def forward(self, modulations):\n",
        "        loss = 0.0  # ⭐ 여기에 초기화\n",
        "\n",
        "        for modulation_set in modulations:\n",
        "            for modulation in modulation_set:\n",
        "                negative = modulation[modulation < 0]\n",
        "                if negative.numel() > 0:\n",
        "                    loss += torch.mean(input=torch.abs(input=negative))\n",
        "\n",
        "        loss = loss * self.weight\n",
        "\n",
        "        # ⭐ Tensor로 변환\n",
        "        return torch.tensor(data=loss, device=modulations[0][0].device, dtype=torch.float32)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udjZU8oZS2fz"
      },
      "source": [
        "## model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ghFZqujLS5DS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import lightning as L\n",
        "\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR, CosineAnnealingWarmRestarts\n",
        "from transformers.optimization import get_cosine_schedule_with_warmup\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### basic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E1dbPGVdS8qE"
      },
      "outputs": [],
      "source": [
        "class HomomorphicDit(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        image_size=512,\n",
        "        hidden_size=768,\n",
        "        patch_size=4,\n",
        "        depth=12,\n",
        "        num_heads=12,\n",
        "        in_channels=3,\n",
        "        out_channels=1,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.image_size = image_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.patch_size = patch_size\n",
        "        self.depth = depth\n",
        "        self.num_heads = num_heads\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "\n",
        "        self.rgb2ycrcb = RGB2YCrCb()\n",
        "        self.homo_separate = HomomorphicSeparation(\n",
        "            size=self.image_size\n",
        "        )\n",
        "        self.illum_embedding = PatchEmbedding(\n",
        "            image_size=self.image_size,\n",
        "            embed_dim=self.hidden_size,\n",
        "            patch_size=self.patch_size,\n",
        "            bias=True\n",
        "        )\n",
        "        self.pos_embedding = PositionalEmbedding(\n",
        "            embed_dim=self.hidden_size,\n",
        "            size=self.illum_embedding.grid_size\n",
        "        )\n",
        "        self.t_embedding = TimeEmbedding(\n",
        "            hidden_size=self.hidden_size,\n",
        "            bias=True\n",
        "        )\n",
        "        self.blocks = nn.ModuleList(modules=[\n",
        "            DiTBlock(\n",
        "                hidden_size=self.hidden_size,\n",
        "                num_heads=self.num_heads,\n",
        "            )\n",
        "            for _ in range(self.depth)\n",
        "        ])\n",
        "        self.final_layer = FinalLayer(\n",
        "            hidden_size=self.hidden_size,\n",
        "            patch_size=self.patch_size,\n",
        "            out_channels=self.out_channels,\n",
        "        )\n",
        "        self.ycrcb2rgb = YCrCb2RGB()\n",
        "        self.initialize_weights()\n",
        "\n",
        "    def initialize_weights(self):\n",
        "        # Initialize transformer layers:\n",
        "        def _basic_init(module):\n",
        "            if isinstance(module, nn.Linear):\n",
        "                torch.nn.init.xavier_uniform_(tensor=module.weight)\n",
        "                if module.bias is not None:\n",
        "                    nn.init.constant_(tensor=module.bias, val=0)\n",
        "        self.apply(fn=_basic_init)\n",
        "\n",
        "        # Initialize patch_embed like nn.Linear (instead of nn.Conv2d):\n",
        "        w_i = self.illum_embedding.proj.weight.data\n",
        "        nn.init.xavier_uniform_(tensor=w_i.view([w_i.shape[0], -1]))\n",
        "        nn.init.constant_(tensor=self.illum_embedding.proj.bias, val=0)\n",
        "\n",
        "        # Initialize timestep embedding MLP:\n",
        "        nn.init.normal_(tensor=self.t_embedding.mlp[0].weight, std=0.02)\n",
        "        nn.init.normal_(tensor=self.t_embedding.mlp[2].weight, std=0.02)\n",
        "\n",
        "        # Zero-out adaLN modulation layers in DiT blocks:\n",
        "        for block in self.blocks:\n",
        "            # nn.init.xavier_uniform_(block.adaLN_modulation[-1].weight)\n",
        "            nn.init.constant_(tensor=block.adaLN_modulation[-1].weight, val=0)\n",
        "            nn.init.constant_(tensor=block.adaLN_modulation[-1].bias, val=0)\n",
        "\n",
        "        # Zero-out output layers:\n",
        "        # nn.init.xavier_uniform_(self.final_layer.adaLN_modulation[-1].weight)\n",
        "        nn.init.constant_(\n",
        "            tensor=self.final_layer.adaLN_modulation[-1].weight, val=0)\n",
        "        nn.init.constant_(\n",
        "            tensor=self.final_layer.adaLN_modulation[-1].bias, val=0)\n",
        "        nn.init.constant_(tensor=self.final_layer.linear.weight, val=0)\n",
        "        nn.init.constant_(tensor=self.final_layer.linear.bias, val=0)\n",
        "\n",
        "    def unpatchify(self, x):\n",
        "        c = self.out_channels\n",
        "        p = self.illum_embedding.patch_size\n",
        "        h = w = int(x=x.shape[1] ** 0.5)\n",
        "\n",
        "        x = x.reshape(shape=(x.shape[0], h, w, p, p, c))\n",
        "        x = torch.einsum('nhwpqc->nchpwq', x)\n",
        "        x = x.reshape(shape=(x.shape[0], c, h * p, h * p))\n",
        "        return x\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        Y, Cr, Cb = self.rgb2ycrcb(x)\n",
        "        i_x, d_x = self.homo_separate(Y)\n",
        "        i_emb = self.illum_embedding(i_x) + self.pos_embedding()\n",
        "\n",
        "        t_emb = self.t_embedding(t)\n",
        "        cond = t_emb\n",
        "\n",
        "        for block in self.blocks:\n",
        "            i_emb = block(i_emb, cond)\n",
        "\n",
        "        i_emb = self.final_layer(i_emb, cond)\n",
        "        i_x = self.unpatchify(x=i_emb)\n",
        "        n_Y = i_x * d_x\n",
        "        dit_enh_img = self.ycrcb2rgb(n_Y, Cr, Cb)\n",
        "        return dit_enh_img\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Feature Attribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class HomomorphicDit(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        image_size=512,\n",
        "        hidden_size=768,\n",
        "        patch_size=4,\n",
        "        depth=12,\n",
        "        num_heads=12,\n",
        "        in_channels=3,\n",
        "        out_channels=1,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.image_size = image_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.patch_size = patch_size\n",
        "        self.depth = depth\n",
        "        self.num_heads = num_heads\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "\n",
        "        self.rgb2ycrcb = RGB2YCrCb()\n",
        "        self.homo_separate = HomomorphicSeparation(\n",
        "            size=self.image_size\n",
        "        )\n",
        "        self.illum_embedding = PatchEmbedding(\n",
        "            image_size=self.image_size,\n",
        "            embed_dim=self.hidden_size,\n",
        "            patch_size=self.patch_size,\n",
        "            bias=True\n",
        "        )\n",
        "        self.pos_embedding = PositionalEmbedding(\n",
        "            embed_dim=self.hidden_size,\n",
        "            size=self.illum_embedding.grid_size\n",
        "        )\n",
        "        self.t_embedding = TimeEmbedding(\n",
        "            hidden_size=self.hidden_size,\n",
        "            bias=True\n",
        "        )\n",
        "        self.blocks = nn.ModuleList(modules=[\n",
        "            DiTBlock(\n",
        "                hidden_size=self.hidden_size,\n",
        "                num_heads=self.num_heads,\n",
        "            )\n",
        "            for _ in range(self.depth)\n",
        "        ])\n",
        "        self.final_layer = FinalLayer(\n",
        "            hidden_size=self.hidden_size,\n",
        "            patch_size=self.patch_size,\n",
        "            out_channels=self.out_channels,\n",
        "        )\n",
        "        self.ycrcb2rgb = YCrCb2RGB()\n",
        "        self.initialize_weights()\n",
        "\n",
        "    def initialize_weights(self):\n",
        "        # Initialize transformer layers:\n",
        "        def _basic_init(module):\n",
        "            if isinstance(module, nn.Linear):\n",
        "                torch.nn.init.xavier_uniform_(tensor=module.weight)\n",
        "                if module.bias is not None:\n",
        "                    nn.init.constant_(tensor=module.bias, val=0)\n",
        "        self.apply(fn=_basic_init)\n",
        "\n",
        "        # Initialize patch_embed like nn.Linear (instead of nn.Conv2d):\n",
        "        w_i = self.illum_embedding.proj.weight.data\n",
        "        nn.init.xavier_uniform_(tensor=w_i.view([w_i.shape[0], -1]))\n",
        "        nn.init.constant_(tensor=self.illum_embedding.proj.bias, val=0)\n",
        "\n",
        "        # Initialize timestep embedding MLP:\n",
        "        nn.init.normal_(tensor=self.t_embedding.mlp[0].weight, std=0.02)\n",
        "        nn.init.normal_(tensor=self.t_embedding.mlp[2].weight, std=0.02)\n",
        "\n",
        "        # Zero-out adaLN modulation layers in DiT blocks:\n",
        "        for block in self.blocks:\n",
        "            # nn.init.xavier_uniform_(block.adaLN_modulation[-1].weight)\n",
        "            nn.init.constant_(tensor=block.adaLN_modulation[-1].weight, val=0)\n",
        "            nn.init.constant_(tensor=block.adaLN_modulation[-1].bias, val=0)\n",
        "\n",
        "        # Zero-out output layers:\n",
        "        # nn.init.xavier_uniform_(self.final_layer.adaLN_modulation[-1].weight)\n",
        "        nn.init.constant_(\n",
        "            tensor=self.final_layer.adaLN_modulation[-1].weight, val=0)\n",
        "        nn.init.constant_(\n",
        "            tensor=self.final_layer.adaLN_modulation[-1].bias, val=0)\n",
        "        nn.init.constant_(tensor=self.final_layer.linear.weight, val=0)\n",
        "        nn.init.constant_(tensor=self.final_layer.linear.bias, val=0)\n",
        "\n",
        "    def unpatchify(self, x):\n",
        "        c = self.out_channels\n",
        "        p = self.illum_embedding.patch_size\n",
        "        h = w = int(x.shape[1] ** 0.5)\n",
        "\n",
        "        x = x.reshape(shape=(x.shape[0], h, w, p, p, c))\n",
        "        x = torch.einsum('nhwpqc->nchpwq', x)\n",
        "        x = x.reshape(shape=(x.shape[0], c, h * p, h * p))\n",
        "        return x\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        Y, Cr, Cb = self.rgb2ycrcb(x)\n",
        "        i_x, d_x = self.homo_separate(Y)\n",
        "        i_emb = self.illum_embedding(i_x) + self.pos_embedding()\n",
        "\n",
        "        t_emb = self.t_embedding(t)\n",
        "        cond = t_emb\n",
        "\n",
        "        modulations = []  # ⭐ modulation 기록할 리스트\n",
        "\n",
        "        for block in self.blocks:\n",
        "            shift_msa, scale_msa, gate_msa, shift_mlp, scale_mlp, gate_mlp = block.adaLN_modulation(\n",
        "                cond).chunk(6, dim=1)\n",
        "            modulations.append(\n",
        "                (shift_msa, scale_msa, gate_msa, shift_mlp, scale_mlp, gate_mlp))\n",
        "            i_emb = block(i_emb, cond)\n",
        "\n",
        "        i_emb = self.final_layer(i_emb, cond)\n",
        "        i_x_out = self.unpatchify(i_emb)\n",
        "        n_Y = i_x_out * d_x\n",
        "        dit_enh_img = self.ycrcb2rgb(n_Y, Cr, Cb)\n",
        "\n",
        "        return dit_enh_img, modulations, i_x, i_x_out  # ⭐\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9STseQ2kMMm"
      },
      "source": [
        "## lightning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### adam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z1m5ERhmkOy6"
      },
      "outputs": [],
      "source": [
        "class HomomorphicDiTLightning(L.LightningModule):\n",
        "    def __init__(self, hparams):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters(hparams)\n",
        "\n",
        "        self.model = HomomorphicDit(\n",
        "            image_size=hparams['image_size'],\n",
        "            hidden_size=hparams['hidden_size'],\n",
        "            patch_size=hparams['patch_size'],\n",
        "            depth=hparams['depth'],\n",
        "            num_heads=hparams['num_heads'],\n",
        "            in_channels=hparams[\"in_channels\"],\n",
        "            out_channels=hparams[\"out_channels\"],\n",
        "        )\n",
        "\n",
        "        self.spa_loss = L_spa()\n",
        "        self.col_loss = L_col()\n",
        "        self.exp_loss = L_exp()\n",
        "\n",
        "        self.lambda_spa = hparams[\"lambda_spa\"]\n",
        "        self.lambda_col = hparams[\"lambda_col\"]\n",
        "        self.lambda_exp = hparams[\"lambda_exp\"]\n",
        "\n",
        "        self.timestep_range = hparams['timestep_range']\n",
        "\n",
        "        self.metric = ImageQualityMetrics(device=\"cuda\")\n",
        "        self.metric.eval()\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        return self.model(x, t)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x = batch.to(self.device)\n",
        "        t = torch.randint(\n",
        "            low=0,\n",
        "            high=self.timestep_range,\n",
        "            size=(x.size(0),),\n",
        "            device=self.device\n",
        "        )\n",
        "        enh_img = self(x, t)\n",
        "\n",
        "        loss_spa = self.lambda_spa * \\\n",
        "            torch.mean(input=self.spa_loss(x, enh_img))\n",
        "        loss_col = self.lambda_col * torch.mean(input=self.col_loss(enh_img))\n",
        "        loss_exp = self.lambda_exp * torch.mean(input=self.exp_loss(enh_img))\n",
        "\n",
        "        total = (loss_spa + loss_col + loss_exp)\n",
        "\n",
        "        self.log_dict(dictionary={\n",
        "            \"train/spa\": loss_spa,\n",
        "            \"train/col\": loss_col,\n",
        "            \"train/exp\": loss_exp,\n",
        "            \"train/total\": total,\n",
        "        }, prog_bar=True)\n",
        "        return total\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x = batch.to(self.device)\n",
        "        t = torch.randint(\n",
        "            low=0,\n",
        "            high=self.timestep_range,\n",
        "            size=(x.size(0),),\n",
        "            device=self.device\n",
        "        )\n",
        "        enh_img = self(x, t)\n",
        "\n",
        "        loss_spa = self.lambda_spa * \\\n",
        "            torch.mean(input=self.spa_loss(x, enh_img))\n",
        "        loss_col = self.lambda_col * torch.mean(input=self.col_loss(enh_img))\n",
        "        loss_exp = self.lambda_exp * torch.mean(input=self.exp_loss(enh_img))\n",
        "\n",
        "        total = (loss_spa + loss_col + loss_exp)\n",
        "\n",
        "        self.log_dict(dictionary={\n",
        "            \"valid/spa\": loss_spa,\n",
        "            \"valid/col\": loss_col,\n",
        "            \"valid/exp\": loss_exp,\n",
        "            \"valid/total\": total,\n",
        "        }, prog_bar=True)\n",
        "        return total\n",
        "\n",
        "    def test_step(self, batch, batch_idx, dataloader_idx=0):\n",
        "        x = batch.to(self.device)\n",
        "        t = torch.zeros(\n",
        "            size=(x.size(0),),\n",
        "            dtype=torch.long,\n",
        "            device=self.device\n",
        "        )\n",
        "        enh_img = self(x, t)\n",
        "\n",
        "        metrics = self.metric.full(preds=enh_img, targets=x)\n",
        "\n",
        "        self.log_dict(dictionary={\n",
        "            \"bench/PSNR\": metrics[\"PSNR\"],\n",
        "            \"bench/SSIM\": metrics[\"SSIM\"],\n",
        "            \"bench/LPIPS\": metrics[\"LPIPS\"],\n",
        "            \"bench/NIQE\": metrics[\"NIQE\"],\n",
        "            \"bench/BRISQUE\": metrics[\"BRISQUE\"],\n",
        "        }, prog_bar=True)\n",
        "        return metrics\n",
        "\n",
        "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
        "        x = batch.to(self.device)\n",
        "        t = torch.zeros(\n",
        "            size=(x.size(0),),\n",
        "            dtype=torch.long,\n",
        "            device=self.device\n",
        "        )\n",
        "        enh_img = self(x, t)\n",
        "        return enh_img\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(\n",
        "            params=self.parameters(),\n",
        "            lr=self.hparams['lr']\n",
        "        )\n",
        "        return optimizer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### get cosine schedule with warmup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class HomomorphicDiTLightning(L.LightningModule):\n",
        "    def __init__(self, hparams):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters(hparams)\n",
        "\n",
        "        self.model = HomomorphicDit(\n",
        "            image_size=hparams['image_size'],\n",
        "            hidden_size=hparams['hidden_size'],\n",
        "            patch_size=hparams['patch_size'],\n",
        "            depth=hparams['depth'],\n",
        "            num_heads=hparams['num_heads'],\n",
        "            in_channels=hparams[\"in_channels\"],\n",
        "            out_channels=hparams[\"out_channels\"],\n",
        "        )\n",
        "\n",
        "        self.spa_loss = L_spa()\n",
        "        self.col_loss = L_col()\n",
        "        self.exp_loss = L_exp()\n",
        "\n",
        "        self.lambda_spa = hparams[\"lambda_spa\"]\n",
        "        self.lambda_col = hparams[\"lambda_col\"]\n",
        "        self.lambda_exp = hparams[\"lambda_exp\"]\n",
        "\n",
        "        self.timestep_range = hparams['timestep_range']\n",
        "\n",
        "        self.metric = ImageQualityMetrics(device=\"cuda\")\n",
        "        self.metric.eval()\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        return self.model(x, t)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x = batch.to(self.device)\n",
        "        t = torch.randint(\n",
        "            low=0,\n",
        "            high=self.timestep_range,\n",
        "            size=(x.size(0),),\n",
        "            device=self.device\n",
        "        )\n",
        "        enh_img = self(x, t)\n",
        "\n",
        "        loss_spa = self.lambda_spa * \\\n",
        "            torch.mean(input=self.spa_loss(x, enh_img))\n",
        "        loss_col = self.lambda_col * torch.mean(input=self.col_loss(enh_img))\n",
        "        loss_exp = self.lambda_exp * torch.mean(input=self.exp_loss(enh_img))\n",
        "\n",
        "        total = (loss_spa + loss_col + loss_exp)\n",
        "\n",
        "        self.log_dict(dictionary={\n",
        "            \"train/spa\": loss_spa,\n",
        "            \"train/col\": loss_col,\n",
        "            \"train/exp\": loss_exp,\n",
        "            \"train/total\": total,\n",
        "        }, prog_bar=True)\n",
        "        return total\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x = batch.to(self.device)\n",
        "        t = torch.randint(\n",
        "            low=0,\n",
        "            high=self.timestep_range,\n",
        "            size=(x.size(0),),\n",
        "            device=self.device\n",
        "        )\n",
        "        enh_img = self(x, t)\n",
        "\n",
        "        loss_spa = self.lambda_spa * \\\n",
        "            torch.mean(input=self.spa_loss(x, enh_img))\n",
        "        loss_col = self.lambda_col * torch.mean(input=self.col_loss(enh_img))\n",
        "        loss_exp = self.lambda_exp * torch.mean(input=self.exp_loss(enh_img))\n",
        "\n",
        "        total = (loss_spa + loss_col + loss_exp)\n",
        "\n",
        "        self.log_dict(dictionary={\n",
        "            \"valid/spa\": loss_spa,\n",
        "            \"valid/col\": loss_col,\n",
        "            \"valid/exp\": loss_exp,\n",
        "            \"valid/total\": total,\n",
        "        }, prog_bar=True)\n",
        "        return total\n",
        "\n",
        "    def test_step(self, batch, batch_idx, dataloader_idx=0):\n",
        "        x = batch.to(self.device)\n",
        "        t = torch.zeros(\n",
        "            size=(x.size(0),),\n",
        "            dtype=torch.long,\n",
        "            device=self.device\n",
        "        )\n",
        "        enh_img = self(x, t)\n",
        "\n",
        "        metrics = self.metric.full(preds=enh_img, targets=x)\n",
        "\n",
        "        self.log_dict(dictionary={\n",
        "            \"bench/PSNR\": metrics[\"PSNR\"],\n",
        "            \"bench/SSIM\": metrics[\"SSIM\"],\n",
        "            \"bench/LPIPS\": metrics[\"LPIPS\"],\n",
        "            \"bench/NIQE\": metrics[\"NIQE\"],\n",
        "            \"bench/BRISQUE\": metrics[\"BRISQUE\"],\n",
        "        }, prog_bar=True)\n",
        "        return metrics\n",
        "\n",
        "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
        "        x = batch.to(self.device)\n",
        "        t = torch.zeros(\n",
        "            size=(x.size(0),),\n",
        "            dtype=torch.long,\n",
        "            device=self.device\n",
        "        )\n",
        "        enh_img = self(x, t)\n",
        "        return enh_img\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        total_steps = self.trainer.estimated_stepping_batches\n",
        "\n",
        "        optimizer = torch.optim.Adam(\n",
        "            params=self.parameters(),\n",
        "            lr=self.hparams['lr']\n",
        "        )\n",
        "\n",
        "        scheduler = get_cosine_schedule_with_warmup(\n",
        "            optimizer=optimizer,\n",
        "            num_warmup_steps=2600,  # 1~2 epoch 분량\n",
        "            num_training_steps=total_steps,\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"optimizer\": optimizer,\n",
        "            \"lr_scheduler\": {\n",
        "                \"scheduler\": scheduler,\n",
        "                \"interval\": \"step\",\n",
        "                \"frequency\": 1,\n",
        "            }\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Cosine Annealing Warm Restarts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class HomomorphicDiTLightning(L.LightningModule):\n",
        "    def __init__(self, hparams):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters(hparams)\n",
        "\n",
        "        self.model = HomomorphicDit(\n",
        "            image_size=hparams['image_size'],\n",
        "            hidden_size=hparams['hidden_size'],\n",
        "            patch_size=hparams['patch_size'],\n",
        "            depth=hparams['depth'],\n",
        "            num_heads=hparams['num_heads'],\n",
        "            in_channels=hparams[\"in_channels\"],\n",
        "            out_channels=hparams[\"out_channels\"],\n",
        "        )\n",
        "\n",
        "        self.spa_loss = L_spa()\n",
        "        self.col_loss = L_col()\n",
        "        self.exp_loss = L_exp()\n",
        "\n",
        "        self.lambda_spa = hparams[\"lambda_spa\"]\n",
        "        self.lambda_col = hparams[\"lambda_col\"]\n",
        "        self.lambda_exp = hparams[\"lambda_exp\"]\n",
        "\n",
        "        self.timestep_range = hparams['timestep_range']\n",
        "\n",
        "        self.metric = ImageQualityMetrics(device=\"cuda\")\n",
        "        self.metric.eval()\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        return self.model(x, t)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x = batch.to(self.device)\n",
        "        t = torch.randint(\n",
        "            low=0,\n",
        "            high=self.timestep_range,\n",
        "            size=(x.size(0),),\n",
        "            device=self.device\n",
        "        )\n",
        "        enh_img = self(x, t)\n",
        "\n",
        "        loss_spa = self.lambda_spa * \\\n",
        "            torch.mean(input=self.spa_loss(x, enh_img))\n",
        "        loss_col = self.lambda_col * torch.mean(input=self.col_loss(enh_img))\n",
        "        loss_exp = self.lambda_exp * torch.mean(input=self.exp_loss(enh_img))\n",
        "\n",
        "        total = (loss_spa + loss_col + loss_exp)\n",
        "\n",
        "        self.log_dict(dictionary={\n",
        "            \"train/spa\": loss_spa,\n",
        "            \"train/col\": loss_col,\n",
        "            \"train/exp\": loss_exp,\n",
        "            \"train/total\": total,\n",
        "        }, prog_bar=True)\n",
        "        return total\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x = batch.to(self.device)\n",
        "        t = torch.randint(\n",
        "            low=0,\n",
        "            high=self.timestep_range,\n",
        "            size=(x.size(0),),\n",
        "            device=self.device\n",
        "        )\n",
        "        enh_img = self(x, t)\n",
        "\n",
        "        loss_spa = self.lambda_spa * \\\n",
        "            torch.mean(input=self.spa_loss(x, enh_img))\n",
        "        loss_col = self.lambda_col * torch.mean(input=self.col_loss(enh_img))\n",
        "        loss_exp = self.lambda_exp * torch.mean(input=self.exp_loss(enh_img))\n",
        "\n",
        "        total = (loss_spa + loss_col + loss_exp)\n",
        "\n",
        "        self.log_dict(dictionary={\n",
        "            \"valid/spa\": loss_spa,\n",
        "            \"valid/col\": loss_col,\n",
        "            \"valid/exp\": loss_exp,\n",
        "            \"valid/total\": total,\n",
        "        }, prog_bar=True)\n",
        "        return total\n",
        "\n",
        "    def test_step(self, batch, batch_idx, dataloader_idx=0):\n",
        "        x = batch.to(self.device)\n",
        "        t = torch.zeros(\n",
        "            size=(x.size(0),),\n",
        "            dtype=torch.long,\n",
        "            device=self.device\n",
        "        )\n",
        "        enh_img = self(x, t)\n",
        "\n",
        "        metrics = self.metric.full(preds=enh_img, targets=x)\n",
        "\n",
        "        self.log_dict(dictionary={\n",
        "            \"bench/PSNR\": metrics[\"PSNR\"],\n",
        "            \"bench/SSIM\": metrics[\"SSIM\"],\n",
        "            \"bench/LPIPS\": metrics[\"LPIPS\"],\n",
        "            \"bench/NIQE\": metrics[\"NIQE\"],\n",
        "            \"bench/BRISQUE\": metrics[\"BRISQUE\"],\n",
        "        }, prog_bar=True)\n",
        "        return metrics\n",
        "\n",
        "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
        "        x = batch.to(self.device)\n",
        "        t = torch.zeros(\n",
        "            size=(x.size(0),),\n",
        "            dtype=torch.long,\n",
        "            device=self.device\n",
        "        )\n",
        "        enh_img = self(x, t)\n",
        "        return enh_img\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(\n",
        "            params=self.parameters(),\n",
        "            lr=self.hparams['lr']\n",
        "        )\n",
        "\n",
        "        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "            optimizer=optimizer,\n",
        "            T_0=10,           # 최초 주기 (epoch 수 기준) - 예를 들어 10 에폭마다 리셋\n",
        "            T_mult=2,         # 주기를 2배로 늘려가면서 반복\n",
        "            eta_min=1e-7      # 최소 학습률\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"optimizer\": optimizer,\n",
        "            \"lr_scheduler\": {\n",
        "                \"scheduler\": scheduler,\n",
        "                \"interval\": \"epoch\",  # 매 에폭마다 스케줄러 업데이트\n",
        "                \"frequency\": 1,\n",
        "            }\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### dynamic reweight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class HomomorphicDiTLightning(L.LightningModule):\n",
        "    def __init__(self, hparams):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters(hparams)\n",
        "\n",
        "        self.model = HomomorphicDit(\n",
        "            image_size=hparams['image_size'],\n",
        "            hidden_size=hparams['hidden_size'],\n",
        "            patch_size=hparams['patch_size'],\n",
        "            depth=hparams['depth'],\n",
        "            num_heads=hparams['num_heads'],\n",
        "            in_channels=hparams[\"in_channels\"],\n",
        "            out_channels=hparams[\"out_channels\"],\n",
        "        )\n",
        "\n",
        "        self.spa_loss = L_spa()\n",
        "        self.col_loss = L_col()\n",
        "        self.exp_loss = L_exp()\n",
        "\n",
        "        self.lambda_spa = hparams[\"lambda_spa\"]\n",
        "        self.lambda_col = hparams[\"lambda_col\"]\n",
        "        self.lambda_exp = hparams[\"lambda_exp\"]\n",
        "\n",
        "        self.timestep_range = hparams['timestep_range']\n",
        "\n",
        "        self.metric = ImageQualityMetrics(device=\"cuda\")\n",
        "        self.metric.eval()\n",
        "\n",
        "    def dynamic_reweight(self, losses):\n",
        "        total = sum(losses.values())\n",
        "        weights = {}\n",
        "        for key, value in losses.items():\n",
        "            normalized = value / (total + 1e-8)\n",
        "            weights[key] = 1.0 / (normalized + 1e-6)\n",
        "\n",
        "        weight_sum = sum(weights.values())\n",
        "        for key in weights.keys():\n",
        "            weights[key] /= weight_sum\n",
        "        return weights\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        return self.model(x, t)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x = batch.to(self.device)\n",
        "        t = torch.randint(\n",
        "            low=0,\n",
        "            high=self.timestep_range,\n",
        "            size=(x.size(0),),\n",
        "            device=self.device\n",
        "        )\n",
        "        enh_img = self(x, t)\n",
        "\n",
        "        loss_spa = torch.mean(self.spa_loss(x, enh_img))\n",
        "        loss_col = torch.mean(self.col_loss(enh_img))\n",
        "        loss_exp = torch.mean(self.exp_loss(enh_img))\n",
        "\n",
        "        # --- Dynamic Reweighting\n",
        "        losses = {\n",
        "            \"spa\": loss_spa.detach(),\n",
        "            \"col\": loss_col.detach(),\n",
        "            \"exp\": loss_exp.detach()\n",
        "        }\n",
        "        dynamic_lambdas = self.dynamic_reweight(losses)\n",
        "\n",
        "        total = (\n",
        "            dynamic_lambdas[\"spa\"] * loss_spa +\n",
        "            dynamic_lambdas[\"col\"] * loss_col +\n",
        "            dynamic_lambdas[\"exp\"] * loss_exp\n",
        "        )\n",
        "\n",
        "        self.log_dict({\n",
        "            \"train/spa\": loss_spa,\n",
        "            \"train/col\": loss_col,\n",
        "            \"train/exp\": loss_exp,\n",
        "            \"train/total\": total,\n",
        "            \"train/weight_spa\": dynamic_lambdas[\"spa\"],\n",
        "            \"train/weight_col\": dynamic_lambdas[\"col\"],\n",
        "            \"train/weight_exp\": dynamic_lambdas[\"exp\"],\n",
        "        }, prog_bar=True)\n",
        "\n",
        "        return total\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x = batch.to(self.device)\n",
        "        t = torch.randint(\n",
        "            low=0,\n",
        "            high=self.timestep_range,\n",
        "            size=(x.size(0),),\n",
        "            device=self.device\n",
        "        )\n",
        "        enh_img = self(x, t)\n",
        "\n",
        "        loss_spa = torch.mean(self.spa_loss(x, enh_img))\n",
        "        loss_col = torch.mean(self.col_loss(enh_img))\n",
        "        loss_exp = torch.mean(self.exp_loss(enh_img))\n",
        "\n",
        "        losses = {\n",
        "            \"spa\": loss_spa.detach(),\n",
        "            \"col\": loss_col.detach(),\n",
        "            \"exp\": loss_exp.detach()\n",
        "        }\n",
        "        dynamic_lambdas = self.dynamic_reweight(losses)\n",
        "\n",
        "        total = (\n",
        "            dynamic_lambdas[\"spa\"] * loss_spa +\n",
        "            dynamic_lambdas[\"col\"] * loss_col +\n",
        "            dynamic_lambdas[\"exp\"] * loss_exp\n",
        "        )\n",
        "\n",
        "        self.log_dict({\n",
        "            \"valid/spa\": loss_spa,\n",
        "            \"valid/col\": loss_col,\n",
        "            \"valid/exp\": loss_exp,\n",
        "            \"valid/total\": total,\n",
        "            \"valid/weight_spa\": dynamic_lambdas[\"spa\"],\n",
        "            \"valid/weight_col\": dynamic_lambdas[\"col\"],\n",
        "            \"valid/weight_exp\": dynamic_lambdas[\"exp\"],\n",
        "        }, prog_bar=True)\n",
        "\n",
        "        return total\n",
        "\n",
        "    def test_step(self, batch, batch_idx, dataloader_idx=0):\n",
        "        x = batch.to(self.device)\n",
        "        t = torch.zeros(\n",
        "            size=(x.size(0),),\n",
        "            dtype=torch.long,\n",
        "            device=self.device\n",
        "        )\n",
        "        enh_img = self(x, t)\n",
        "\n",
        "        metrics = self.metric.full(enh_img, x)\n",
        "\n",
        "        self.log_dict({\n",
        "            \"bench/PSNR\": metrics[\"PSNR\"],\n",
        "            \"bench/SSIM\": metrics[\"SSIM\"],\n",
        "            \"bench/LPIPS\": metrics[\"LPIPS\"],\n",
        "            \"bench/NIQE\": metrics[\"NIQE\"],\n",
        "            \"bench/BRISQUE\": metrics[\"BRISQUE\"],\n",
        "        }, prog_bar=True)\n",
        "        return metrics\n",
        "\n",
        "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
        "        x = batch.to(self.device)\n",
        "        t = torch.zeros(\n",
        "            size=(x.size(0),),\n",
        "            dtype=torch.long,\n",
        "            device=self.device\n",
        "        )\n",
        "        enh_img = self(x, t)\n",
        "        return enh_img\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(\n",
        "            params=self.parameters(),\n",
        "            lr=self.hparams['lr']\n",
        "        )\n",
        "\n",
        "        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "            optimizer=optimizer,\n",
        "            T_0=10,           # 최초 주기 (epoch 수 기준) - 예를 들어 10 에폭마다 리셋\n",
        "            T_mult=2,         # 주기를 2배로 늘려가면서 반복\n",
        "            eta_min=1e-7      # 최소 학습률\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"optimizer\": optimizer,\n",
        "            \"lr_scheduler\": {\n",
        "                \"scheduler\": scheduler,\n",
        "                \"interval\": \"epoch\",  # 매 에폭마다 스케줄러 업데이트\n",
        "                \"frequency\": 1,\n",
        "            }\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Feature Attribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class HomomorphicDiTLightning(L.LightningModule):\n",
        "    def __init__(self, hparams):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters(hparams)\n",
        "\n",
        "        self.model = HomomorphicDit(\n",
        "            image_size=hparams['image_size'],\n",
        "            hidden_size=hparams['hidden_size'],\n",
        "            patch_size=hparams['patch_size'],\n",
        "            depth=hparams['depth'],\n",
        "            num_heads=hparams['num_heads'],\n",
        "            in_channels=hparams[\"in_channels\"],\n",
        "            out_channels=hparams[\"out_channels\"],\n",
        "        )\n",
        "\n",
        "        self.spa_loss = L_spa()\n",
        "        self.col_loss = L_col()\n",
        "        self.exp_loss = L_exp()\n",
        "        self.bri_loss = L_bri()\n",
        "        self.mod_loss = L_mod()\n",
        "\n",
        "        self.lambda_spa = hparams[\"lambda_spa\"]\n",
        "        self.lambda_col = hparams[\"lambda_col\"]\n",
        "        self.lambda_exp = hparams[\"lambda_exp\"]\n",
        "        self.lambda_bri = hparams[\"lambda_bri\"]\n",
        "        self.lambda_mod = hparams[\"lambda_mod\"]\n",
        "\n",
        "        self.timestep_range = hparams['timestep_range']\n",
        "\n",
        "        self.metric = ImageQualityMetrics(device=\"cuda\")\n",
        "        self.metric.eval()\n",
        "\n",
        "    def dynamic_reweight(self, losses):\n",
        "        total = sum(losses.values())\n",
        "        weights = {}\n",
        "        for key, value in losses.items():\n",
        "            normalized = value / (total + 1e-8)\n",
        "            weights[key] = 1.0 / (normalized + 1e-6)\n",
        "\n",
        "        weight_sum = sum(weights.values())\n",
        "        for key in weights.keys():\n",
        "            weights[key] /= weight_sum\n",
        "        return weights\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        return self.model(x, t)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x = batch.to(self.device)\n",
        "        t = torch.randint(\n",
        "            low=0,\n",
        "            high=self.timestep_range,\n",
        "            size=(x.size(0),),\n",
        "            device=self.device\n",
        "        )\n",
        "        enh_img, modulations, illumination_in, illumination_out = self(x, t)\n",
        "\n",
        "        loss_spa = torch.mean(input=self.spa_loss(x, enh_img))\n",
        "        loss_col = torch.mean(input=self.col_loss(enh_img))\n",
        "        loss_exp = torch.mean(input=self.exp_loss(enh_img))\n",
        "\n",
        "        loss_bri = torch.mean(\n",
        "            input=self.bri_loss(\n",
        "                illumination_in,\n",
        "                illumination_out,\n",
        "                t\n",
        "            )\n",
        "        )\n",
        "        loss_mod = torch.mean(\n",
        "            input=self.mod_loss(\n",
        "                modulations\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # --- Dynamic Reweighting\n",
        "        losses = {\n",
        "            \"spa\": loss_spa.detach(),\n",
        "            \"col\": loss_col.detach(),\n",
        "            \"exp\": loss_exp.detach(),\n",
        "            \"bri\": loss_bri.detach(),\n",
        "            \"mod\": loss_mod.detach(),\n",
        "        }\n",
        "        dynamic_lambdas = self.dynamic_reweight(losses=losses)\n",
        "\n",
        "        total = (\n",
        "            dynamic_lambdas[\"spa\"] * loss_spa +\n",
        "            dynamic_lambdas[\"col\"] * loss_col +\n",
        "            dynamic_lambdas[\"exp\"] * loss_exp +\n",
        "            dynamic_lambdas[\"bri\"] * loss_bri +\n",
        "            dynamic_lambdas[\"mod\"] * loss_mod\n",
        "        )\n",
        "\n",
        "        self.log_dict(\n",
        "            dictionary={\n",
        "                \"train/01_spa\": loss_spa,\n",
        "                \"train/02_col\": loss_col,\n",
        "                \"train/03_exp\": loss_exp,\n",
        "                \"train/04_bri\": loss_bri,\n",
        "                \"train/05_mod\": loss_mod,\n",
        "                \"train/06_total\": total,\n",
        "                \"train/07_weight_spa\": dynamic_lambdas[\"spa\"],\n",
        "                \"train/08_weight_col\": dynamic_lambdas[\"col\"],\n",
        "                \"train/09_weight_exp\": dynamic_lambdas[\"exp\"],\n",
        "                \"train/10_weight_bri\": dynamic_lambdas[\"bri\"],\n",
        "                \"train/11_weight_mod\": dynamic_lambdas[\"mod\"],\n",
        "            },\n",
        "            prog_bar=True\n",
        "        )\n",
        "\n",
        "        return total\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x = batch.to(self.device)\n",
        "        t = torch.randint(\n",
        "            low=0,\n",
        "            high=self.timestep_range,\n",
        "            size=(x.size(0),),\n",
        "            device=self.device\n",
        "        )\n",
        "        enh_img, modulations, illumination_in, illumination_out = self(x, t)\n",
        "\n",
        "        loss_spa = torch.mean(input=self.spa_loss(x, enh_img))\n",
        "        loss_col = torch.mean(input=self.col_loss(enh_img))\n",
        "        loss_exp = torch.mean(input=self.exp_loss(enh_img))\n",
        "\n",
        "        loss_bri = torch.mean(\n",
        "            input=self.bri_loss(\n",
        "                illumination_in,\n",
        "                illumination_out,\n",
        "                t\n",
        "            )\n",
        "        )\n",
        "        loss_mod = torch.mean(\n",
        "            input=self.mod_loss(\n",
        "                modulations\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # --- Dynamic Reweighting\n",
        "        losses = {\n",
        "            \"spa\": loss_spa.detach(),\n",
        "            \"col\": loss_col.detach(),\n",
        "            \"exp\": loss_exp.detach(),\n",
        "            \"bri\": loss_bri.detach(),\n",
        "            \"mod\": loss_mod.detach(),\n",
        "        }\n",
        "        dynamic_lambdas = self.dynamic_reweight(losses=losses)\n",
        "\n",
        "        total = (\n",
        "            dynamic_lambdas[\"spa\"] * loss_spa +\n",
        "            dynamic_lambdas[\"col\"] * loss_col +\n",
        "            dynamic_lambdas[\"exp\"] * loss_exp +\n",
        "            dynamic_lambdas[\"bri\"] * loss_bri +\n",
        "            dynamic_lambdas[\"mod\"] * loss_mod\n",
        "        )\n",
        "\n",
        "        self.log_dict(\n",
        "            dictionary={\n",
        "                \"valid/01_spa\": loss_spa,\n",
        "                \"valid/02_col\": loss_col,\n",
        "                \"valid/03_exp\": loss_exp,\n",
        "                \"valid/04_bri\": loss_bri,\n",
        "                \"valid/05_mod\": loss_mod,\n",
        "                \"valid/06_total\": total,\n",
        "                \"valid/07_weight_spa\": dynamic_lambdas[\"spa\"],\n",
        "                \"valid/08_weight_col\": dynamic_lambdas[\"col\"],\n",
        "                \"valid/09_weight_exp\": dynamic_lambdas[\"exp\"],\n",
        "                \"valid/10_weight_bri\": dynamic_lambdas[\"bri\"],\n",
        "                \"valid/11_weight_mod\": dynamic_lambdas[\"mod\"],\n",
        "            },\n",
        "            prog_bar=True\n",
        "        )\n",
        "\n",
        "        return total\n",
        "\n",
        "    def test_step(self, batch, batch_idx, dataloader_idx=0):\n",
        "        x = batch.to(self.device)\n",
        "        t = torch.zeros(\n",
        "            size=(x.size(0),),\n",
        "            dtype=torch.long,\n",
        "            device=self.device\n",
        "        )\n",
        "        enh_img, modulations, illumination_in, illumination_out = self(x, t)\n",
        "\n",
        "        metrics = self.metric.full(preds=enh_img, targets=x)\n",
        "\n",
        "        self.log_dict(\n",
        "            dictionary={\n",
        "                \"bench/01_PSNR\": metrics[\"PSNR\"],\n",
        "                \"bench/02_SSIM\": metrics[\"SSIM\"],\n",
        "                \"bench/03_LPIPS\": metrics[\"LPIPS\"],\n",
        "                \"bench/04_NIQE\": metrics[\"NIQE\"],\n",
        "                \"bench/05_BRISQUE\": metrics[\"BRISQUE\"],\n",
        "            },\n",
        "            prog_bar=True\n",
        "        )\n",
        "        return metrics\n",
        "\n",
        "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
        "        x = batch.to(self.device)\n",
        "        t = torch.zeros(\n",
        "            size=(x.size(0),),\n",
        "            dtype=torch.long,\n",
        "            device=self.device\n",
        "        )\n",
        "        enh_img, modulations, illumination_in, illumination_out = self(x, t)\n",
        "        return enh_img\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(\n",
        "            params=self.parameters(),\n",
        "            lr=self.hparams['lr']\n",
        "        )\n",
        "\n",
        "        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "            optimizer=optimizer,\n",
        "            T_0=10,           # 최초 주기 (epoch 수 기준) - 예를 들어 10 에폭마다 리셋\n",
        "            T_mult=2,         # 주기를 2배로 늘려가면서 반복\n",
        "            eta_min=1e-7      # 최소 학습률\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"optimizer\": optimizer,\n",
        "            \"lr_scheduler\": {\n",
        "                \"scheduler\": scheduler,\n",
        "                \"interval\": \"epoch\",  # 매 에폭마다 스케줄러 업데이트\n",
        "                \"frequency\": 1,\n",
        "            }\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class HomomorphicDiTLightning(L.LightningModule):\n",
        "    def __init__(self, hparams):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters(hparams)\n",
        "\n",
        "        self.model = HomomorphicDit(\n",
        "            image_size=hparams['image_size'],\n",
        "            hidden_size=hparams['hidden_size'],\n",
        "            patch_size=hparams['patch_size'],\n",
        "            depth=hparams['depth'],\n",
        "            num_heads=hparams['num_heads'],\n",
        "            in_channels=hparams[\"in_channels\"],\n",
        "            out_channels=hparams[\"out_channels\"],\n",
        "        )\n",
        "\n",
        "        self.spa_loss = L_spa()\n",
        "        self.col_loss = L_col()\n",
        "        self.exp_loss = L_exp()\n",
        "        self.bri_loss = L_bri()\n",
        "        self.mod_loss = L_mod()\n",
        "\n",
        "        self.lambda_spa = hparams[\"lambda_spa\"]\n",
        "        self.lambda_col = hparams[\"lambda_col\"]\n",
        "        self.lambda_exp = hparams[\"lambda_exp\"]\n",
        "        self.lambda_bri = hparams[\"lambda_bri\"]\n",
        "        self.lambda_mod = hparams[\"lambda_mod\"]\n",
        "\n",
        "        self.timestep_range = hparams['timestep_range']\n",
        "\n",
        "        self.metric = ImageQualityMetrics(device=\"cuda\")\n",
        "        self.metric.eval()\n",
        "\n",
        "    def dynamic_reweight(self, losses):\n",
        "        total = sum(losses.values())\n",
        "        weights = {}\n",
        "        for key, value in losses.items():\n",
        "            normalized = value / (total + 1e-8)\n",
        "            weights[key] = 1.0 / (normalized + 1e-6)\n",
        "\n",
        "        weight_sum = sum(weights.values())\n",
        "        for key in weights.keys():\n",
        "            weights[key] /= weight_sum\n",
        "        return weights\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        return self.model(x, t)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x = batch.to(self.device)\n",
        "        t = torch.randint(\n",
        "            low=0,\n",
        "            high=self.timestep_range,\n",
        "            size=(x.size(0),),\n",
        "            device=self.device\n",
        "        )\n",
        "        dit_enh_img, modulations, i_x, i_x_out = self(x, t)\n",
        "\n",
        "        loss_spa = torch.mean(input=self.spa_loss(x, dit_enh_img))\n",
        "        loss_col = torch.mean(input=self.col_loss(dit_enh_img))\n",
        "        loss_exp = torch.mean(input=self.exp_loss(dit_enh_img))\n",
        "\n",
        "        loss_bri = torch.mean(input=self.bri_loss(i_x, i_x_out, t))\n",
        "        loss_mod = torch.mean(input=self.mod_loss(modulations))\n",
        "\n",
        "        # --- Dynamic Reweighting\n",
        "        losses = {\n",
        "            \"spa\": loss_spa.detach(),\n",
        "            \"col\": loss_col.detach(),\n",
        "            \"exp\": loss_exp.detach(),\n",
        "            \"bri\": loss_bri.detach(),\n",
        "            \"mod\": loss_mod.detach(),\n",
        "        }\n",
        "        dynamic_lambdas = self.dynamic_reweight(losses=losses)\n",
        "\n",
        "        total = (\n",
        "            dynamic_lambdas[\"spa\"] * loss_spa +\n",
        "            dynamic_lambdas[\"col\"] * loss_col +\n",
        "            dynamic_lambdas[\"exp\"] * loss_exp +\n",
        "            dynamic_lambdas[\"bri\"] * loss_bri +\n",
        "            dynamic_lambdas[\"mod\"] * loss_mod\n",
        "        )\n",
        "\n",
        "        self.log_dict(\n",
        "            dictionary={\n",
        "                \"train/01_spa\": loss_spa,\n",
        "                \"train/02_col\": loss_col,\n",
        "                \"train/03_exp\": loss_exp,\n",
        "                \"train/04_bri\": loss_bri,\n",
        "                \"train/05_mod\": loss_mod,\n",
        "                \"train/06_total\": total,\n",
        "                \"train/07_weight_spa\": dynamic_lambdas[\"spa\"],\n",
        "                \"train/08_weight_col\": dynamic_lambdas[\"col\"],\n",
        "                \"train/09_weight_exp\": dynamic_lambdas[\"exp\"],\n",
        "                \"train/10_weight_bri\": dynamic_lambdas[\"bri\"],\n",
        "                \"train/11_weight_mod\": dynamic_lambdas[\"mod\"],\n",
        "            },\n",
        "            prog_bar=True\n",
        "        )\n",
        "\n",
        "        return total\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x = batch.to(self.device)\n",
        "        t = torch.randint(\n",
        "            low=0,\n",
        "            high=self.timestep_range,\n",
        "            size=(x.size(0),),\n",
        "            device=self.device\n",
        "        )\n",
        "        dit_enh_img, modulations, i_x, i_x_out = self(x, t)\n",
        "\n",
        "        loss_spa = torch.mean(input=self.spa_loss(x, dit_enh_img))\n",
        "        loss_col = torch.mean(input=self.col_loss(dit_enh_img))\n",
        "        loss_exp = torch.mean(input=self.exp_loss(dit_enh_img))\n",
        "\n",
        "        loss_bri = torch.mean(\n",
        "            input=self.bri_loss(\n",
        "                i_x,\n",
        "                i_x_out,\n",
        "                t\n",
        "            )\n",
        "        )\n",
        "        loss_mod = torch.mean(\n",
        "            input=self.mod_loss(\n",
        "                modulations\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # --- Dynamic Reweighting\n",
        "        losses = {\n",
        "            \"spa\": loss_spa.detach(),\n",
        "            \"col\": loss_col.detach(),\n",
        "            \"exp\": loss_exp.detach(),\n",
        "            \"bri\": loss_bri.detach(),\n",
        "            \"mod\": loss_mod.detach(),\n",
        "        }\n",
        "        dynamic_lambdas = self.dynamic_reweight(losses=losses)\n",
        "\n",
        "        total = (\n",
        "            dynamic_lambdas[\"spa\"] * loss_spa +\n",
        "            dynamic_lambdas[\"col\"] * loss_col +\n",
        "            dynamic_lambdas[\"exp\"] * loss_exp +\n",
        "            dynamic_lambdas[\"bri\"] * loss_bri +\n",
        "            dynamic_lambdas[\"mod\"] * loss_mod\n",
        "        )\n",
        "\n",
        "        self.log_dict(\n",
        "            dictionary={\n",
        "                \"valid/01_spa\": loss_spa,\n",
        "                \"valid/02_col\": loss_col,\n",
        "                \"valid/03_exp\": loss_exp,\n",
        "                \"valid/04_bri\": loss_bri,\n",
        "                \"valid/05_mod\": loss_mod,\n",
        "                \"valid/06_total\": total,\n",
        "                \"valid/07_weight_spa\": dynamic_lambdas[\"spa\"],\n",
        "                \"valid/08_weight_col\": dynamic_lambdas[\"col\"],\n",
        "                \"valid/09_weight_exp\": dynamic_lambdas[\"exp\"],\n",
        "                \"valid/10_weight_bri\": dynamic_lambdas[\"bri\"],\n",
        "                \"valid/11_weight_mod\": dynamic_lambdas[\"mod\"],\n",
        "            },\n",
        "            prog_bar=True\n",
        "        )\n",
        "\n",
        "        return total\n",
        "\n",
        "    def test_step(self, batch, batch_idx, dataloader_idx=0):\n",
        "        x = batch.to(self.device)\n",
        "        t = torch.zeros(\n",
        "            size=(x.size(0),),\n",
        "            dtype=torch.long,\n",
        "            device=self.device\n",
        "        )\n",
        "        dit_enh_img, modulations, i_x, i_x_out = self(x, t)\n",
        "\n",
        "        metrics = self.metric.full(preds=dit_enh_img, targets=x)\n",
        "\n",
        "        self.log_dict(\n",
        "            dictionary={\n",
        "                \"bench/01_PSNR\": metrics[\"PSNR\"],\n",
        "                \"bench/02_SSIM\": metrics[\"SSIM\"],\n",
        "                \"bench/03_LPIPS\": metrics[\"LPIPS\"],\n",
        "                \"bench/04_NIQE\": metrics[\"NIQE\"],\n",
        "                \"bench/05_BRISQUE\": metrics[\"BRISQUE\"],\n",
        "            },\n",
        "            prog_bar=True\n",
        "        )\n",
        "        return metrics\n",
        "\n",
        "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
        "        x = batch.to(self.device)\n",
        "        t = torch.zeros(\n",
        "            size=(x.size(0),),\n",
        "            dtype=torch.long,\n",
        "            device=self.device\n",
        "        )\n",
        "        dit_enh_img, modulations, i_x, i_x_out = self(x, t)\n",
        "        return dit_enh_img\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(\n",
        "            params=self.parameters(),\n",
        "            lr=self.hparams['lr']\n",
        "        )\n",
        "\n",
        "        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "            optimizer=optimizer,\n",
        "            T_0=10,           # 최초 주기 (epoch 수 기준) - 예를 들어 10 에폭마다 리셋\n",
        "            T_mult=2,         # 주기를 2배로 늘려가면서 반복\n",
        "            eta_min=1e-7      # 최소 학습률\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"optimizer\": optimizer,\n",
        "            \"lr_scheduler\": {\n",
        "                \"scheduler\": scheduler,\n",
        "                \"interval\": \"epoch\",  # 매 에폭마다 스케줄러 업데이트\n",
        "                \"frequency\": 1,\n",
        "            }\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrVAvx9ejOAe"
      },
      "source": [
        "# main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eFm-FY4njQcg"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import torch\n",
        "\n",
        "from pathlib import Path\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## hparams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nfgkd1tmjOw4"
      },
      "outputs": [],
      "source": [
        "def get_hparams():\n",
        "    hparams = {\n",
        "        # 모델 구조\n",
        "        \"image_size\": 512,\n",
        "        \"hidden_size\": 768,\n",
        "        \"patch_size\": 16,  # 32\n",
        "        \"depth\": 12,\n",
        "        \"num_heads\": 12,\n",
        "        \"in_channels\": 3,\n",
        "        \"out_channels\": 1,\n",
        "\n",
        "        # 손실 함수 가중치 (losses.py 기준)\n",
        "        \"lambda_col\": 10.0,\n",
        "        \"lambda_exp\": 0,\n",
        "        \"lambda_spa\": 1000.0,\n",
        "\n",
        "        \"lambda_bri\": 0,\n",
        "        \"lambda_mod\": 1.0,\n",
        "\n",
        "        # timestep range (diffusion time embedding)\n",
        "        \"timestep_range\": 10000,\n",
        "\n",
        "        # 최적화 및 학습 설정\n",
        "        \"lr\": 1e-4,\n",
        "        \"epochs\": 100,\n",
        "        \"batch_size\": 16,\n",
        "        \"seed\": random.randint(a=0, b=1000),\n",
        "\n",
        "        # 데이터 경로\n",
        "        \"train_data_path\": \"data/1_train\",\n",
        "        \"valid_data_path\": \"data/2_valid\",\n",
        "        \"bench_data_path\": \"data/3_bench\",\n",
        "        \"infer_data_path\": \"data/4_infer\",\n",
        "\n",
        "        # 로깅 설정\n",
        "        \"log_dir\": \"./runs2\",\n",
        "        \"experiment_name\": \"HomomorphicDiT\",\n",
        "        \"inference\": \"inference\"\n",
        "    }\n",
        "    return hparams\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "siMqkaqnj0Yr"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    global hparams, model_class, lightning_trainer\n",
        "\n",
        "    model_class = HomomorphicDiTLightning\n",
        "    hparams = get_hparams()\n",
        "\n",
        "    print(\"[RUNNING] Trainer...\")\n",
        "    trainer = LightningTrainer(\n",
        "        model=model_class(hparams=hparams),\n",
        "        hparams=hparams\n",
        "    )\n",
        "    lightning_trainer = trainer.trainer\n",
        "    trainer.run()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "udr-tckWj2Ej"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[RUNNING] Trainer...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Seed set to 127\n",
            "/home/user/anaconda3/envs/jih_icicic/lib/python3.10/site-packages/lightning/fabric/connector.py:571: `precision=16` is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
            "Using 16bit Automatic Mixed Precision (AMP)\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "You are using a CUDA device ('NVIDIA RTX A6000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Start training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
              "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name     </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type                </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>┃\n",
              "┡━━━╇━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
              "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ model    │ HomomorphicDit      │  129 M │ train │\n",
              "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ spa_loss │ L_spa               │     36 │ train │\n",
              "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ col_loss │ L_col               │      0 │ train │\n",
              "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>│ exp_loss │ L_exp               │      0 │ train │\n",
              "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4 </span>│ bri_loss │ L_bri               │      0 │ train │\n",
              "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 5 </span>│ mod_loss │ L_mod               │      0 │ train │\n",
              "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 6 </span>│ metric   │ ImageQualityMetrics │  724 K │ eval  │\n",
              "└───┴──────────┴─────────────────────┴────────┴───────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━┳━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
              "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName    \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType               \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m┃\n",
              "┡━━━╇━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
              "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ model    │ HomomorphicDit      │  129 M │ train │\n",
              "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ spa_loss │ L_spa               │     36 │ train │\n",
              "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ col_loss │ L_col               │      0 │ train │\n",
              "│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ exp_loss │ L_exp               │      0 │ train │\n",
              "│\u001b[2m \u001b[0m\u001b[2m4\u001b[0m\u001b[2m \u001b[0m│ bri_loss │ L_bri               │      0 │ train │\n",
              "│\u001b[2m \u001b[0m\u001b[2m5\u001b[0m\u001b[2m \u001b[0m│ mod_loss │ L_mod               │      0 │ train │\n",
              "│\u001b[2m \u001b[0m\u001b[2m6\u001b[0m\u001b[2m \u001b[0m│ metric   │ ImageQualityMetrics │  724 K │ eval  │\n",
              "└───┴──────────┴─────────────────────┴────────┴───────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 129 M                                                                                            \n",
              "<span style=\"font-weight: bold\">Non-trainable params</span>: 724 K                                                                                        \n",
              "<span style=\"font-weight: bold\">Total params</span>: 130 M                                                                                                \n",
              "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 522                                                                        \n",
              "<span style=\"font-weight: bold\">Modules in train mode</span>: 230                                                                                         \n",
              "<span style=\"font-weight: bold\">Modules in eval mode</span>: 105                                                                                          \n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mTrainable params\u001b[0m: 129 M                                                                                            \n",
              "\u001b[1mNon-trainable params\u001b[0m: 724 K                                                                                        \n",
              "\u001b[1mTotal params\u001b[0m: 130 M                                                                                                \n",
              "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 522                                                                        \n",
              "\u001b[1mModules in train mode\u001b[0m: 230                                                                                         \n",
              "\u001b[1mModules in eval mode\u001b[0m: 105                                                                                          \n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b854efa305304c5f8f381938418bb3a7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Output()"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/tmp/ipykernel_3074143/494960955.py:36: UserWarning: To copy construct from a tensor, it is recommended to use \n",
              "sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than \n",
              "torch.tensor(sourceTensor).\n",
              "  return torch.tensor(data=loss, device=modulations[0][0].device, dtype=torch.float32)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "/tmp/ipykernel_3074143/494960955.py:36: UserWarning: To copy construct from a tensor, it is recommended to use \n",
              "sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than \n",
              "torch.tensor(sourceTensor).\n",
              "  return torch.tensor(data=loss, device=modulations[0][0].device, dtype=torch.float32)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Metric valid/06_total improved. New best score: 0.000\n"
          ]
        }
      ],
      "source": [
        "main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[RUNNING] Inferencer...\n",
            "save_dir: runs2/HomomorphicDiT/version_2/inference\n",
            "[INFO] Start training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Restoring states from the checkpoint path at runs2/HomomorphicDiT/version_2/checkpoints/best-epoch=00.ckpt\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
            "Loaded model weights from the checkpoint at runs2/HomomorphicDiT/version_2/checkpoints/best-epoch=00.ckpt\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5eb9da657e4345e0998a1d87042207a5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Output()"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Inference completed.\n"
          ]
        }
      ],
      "source": [
        "path = Path(f\"{lightning_trainer.log_dir}\")\n",
        "ckpts = path.glob(pattern=\"checkpoints/best*.ckpt\")\n",
        "hparams = path.glob(pattern=\"hparams.yaml\")\n",
        "\n",
        "for ckpt, hparam in zip(ckpts, hparams):\n",
        "    print(\"[RUNNING] Inferencer...\")\n",
        "    inferencer = LightningInferencer(\n",
        "        model=model_class,\n",
        "        trainer=lightning_trainer,\n",
        "        ckpt=ckpt,\n",
        "        hparams=hparam,\n",
        "    )\n",
        "    inferencer.run()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for ckpt, hparam in zip(ckpts, hparams):\n",
        "    print(\"[RUNNING] Inferencer...\")\n",
        "    inferencer = LightningBenchmarker(\n",
        "        model=model_class,\n",
        "        trainer=lightning_trainer,\n",
        "        ckpt=ckpt,\n",
        "        hparams=hparam,\n",
        "    )\n",
        "    inferencer.run()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "jih_icicic",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
